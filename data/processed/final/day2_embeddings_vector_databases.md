Title: DAY 2 Livestream - 5-Day Gen AI Intensive Course | Kaggle
Channel: Kaggle
URL: https://www.youtube.com/watch?v=AjpjCHdIINU
Caption Type: auto-generated

======================================================================

## Cleaned Transcript

Welcome back to the second day of our Kaggle Generative AI intensive course. We're really excited to have you over 250, 000 developers worldwide here to learn today all about AI, about the Gemini models, about AI Studio, about uh Vert. Ex AI, and all of the other great products that Google has been releasing over the last several months. Um, we're overjoyed to to see the responses in YouTube, Discord, and all of the places. and we can't wait to tell you today all about embeddings and vector databases. Um, so just as a quick reminder, our course overview, um, this is a five-day generative AI intensive course brought to you by the Kaggle team in collaboration with Google Cloud and Google DeepMind. Um, it is, um, a kind of a very quick round trip through all of the all of the things that you might have heard about in the generative AI space. And today we are on day two, embeddings and vector databases. If you would like to learn more about foundational models and prompt engineering, please refer to all of the content that we shared yesterday um as well as the YouTube recording. You should be able to access both of those um through all of the all of the content that we've that we've shared on Kaggle. Um my name is Paige Bailey. I lead developer relations engineering at Google DeepMind. Um, as a quick reminder, the content that you should have already been referring to are the the p uh the summary podcast episode, which was coincidentally generated by Notebook LM. Um, we'll be talking to the notebook LM team a little bit later this week, the embeddings and vector stores databases white paper. Um, as well as a few code labs on Kaggle that Anot will be going through later in this section. And um to give a quick overview of the white papers, I'm now going to uh hand it over to Anant. Um after quickly thanking all of our wonderful moderators. If you see them on Discord, they are working tirelessly around the clock to answer your questions um to make sure that you have everything that you need in order to be successful for the course. So please uh please give them uh a lot of emoji love um and say thank you. Um, I especially want to call out Brenda Flynn, um, as well as Kenjal. Um, they have been kind of doing all of the production and stage management behind the scenes and they're really making this course happen. So, thank you so much to all of our wonderful moderators. Um, and with that, uh, Anant, do you want to take over the quick curriculum overview? Sure thing, Paige. Hi, everyone. Um uh it's me again to give you an overview of what we are learning together or what we learned today in today's white papers to kind of set the stage for the Q&A session which a lot of good questions uh you've asked as well. So um today's white paper day two's white paper was about embeddings and vector databases and roughly um we started off by seeing uh with learning about embeddings what embeddings are how they're useful and uh we learned that they are basically numerical vector representations of data let's say text images and more they map diverse data into common semantic space where distance can be used as a proxy for semantic similarity Now this is great for comparing diff different pieces of data or even the same pieces of data uh efficiently or same or similar pieces of data efficiently and they can also be used as a rich semantic representation for downstream models tasks and applications. We also looked at the various types of embeddings such as text embeddings which evolved from early methods like wordtovec all the way to contextaware models like bird and the gemini backbone uh based embeddings. Uh we also have uh image and multimodal embeddings plus also their ways to embed uh structured data and graphs into their uh semantic representations. Then we looked at how training for embeddings often uses dual encoders and contrastive laws to group similar items. This is one method to train embeddings. There's also others. And then evaluating the quality of embeddings uh using metrics like precision and recall as well as um others on standard benchmarks is quite important when doing so. We then looked at how we can search through billions of these vectors quickly. So hint it's not with linear search right. We we also saw how we can use vector search with approximate nearest neighbor algorithms. Uh some of them which we looked into the uh which we read about in the paper uh include scan as well as uh hnsw which are um two very popular algorithms and these basically these ann algorithms trade a tiny bit of accuracy for massive speed gains. Uh and then we also saw how these algorithms are leveraged by vector databases which are built specifically to store, manage and query these embeddings efficiently at scale. We then concluded the white paper by talking about applications of all of these technologies and we looked at various things like how embeddings can be used for search recommendations for um ra or retrieval augmented generation uh based workflows together with LLMs and um we can also um uh we we also saw how um the the embedding part provides the context it's used in the retrieval stage of the RA pipeline to generate more factual and grounded answers. So these things are powerful and I'm uh we'll cover more in the code lab. I'm handing it over to Paige for the Q&A session. Take it over, Paige. Yep, absolutely. I I think that many people have had um uh strong success using vector DBs and embeddings and uh setting up retrieval systems, especially for for building chat bots externally. So, it's it's great to give people the tools in their toolkit in order to do all of this successfully. Um, and so with that, I am going to head into the Q&A. I am so excited and overjoyed today to to welcome to the stage um many folks from um from Google Cloud's Office of the CTO as well as from Google DeepMind. Um, I'm jazzed today to have Patricia, Allan, Shashi, um, Howard, Andre, Chuck, um, and Anant, uh, to to talk through some of the questions that y'all have asked, as well as to give insight into all of the awesome things that they're building with embeddings and vector databases, both in research and out in the external world. Um, so first question is for Andre and for Howard. Um, creating effective embeddings is crucial as we saw in many of the things that we were reading uh over this past 24 hours. What are the latest advancements or best practices in generating embeddings that capture nuanced semantic meaning especially for specialized domains or multimodal data which are increasingly becoming important um uh being able to embed things like images or videos. Um, how much does the choice of the embedding model impact the performance of the downstream vector search task? And Andre, do you want to start? Yes. Yes. Thanks, Paige. Hi, everyone. Thanks for the question. So, to start with the advancements, a notable one recently is to integrate LLMs in the embedding model development stage as I guess you would expect. So, LLMs can be used as a pre-trained backbone to initialize the embedding model and this allows the embedding model to already leverage multilingual and multimodal understanding and not necessarily have the have the embedding model uh uh need to learn multiple languages or multiple modalities directly at the embedding training stage. And the second point about LLM usage is that it can help refine training data sets with data curation or by generating high quality training examples. So I encourage you to take a look at the recent Gemini embedding model and the associated tech report which discusses these points in detail. Now to talk about model specifically talking about embeddings which understand images spatial awareness is a property that's often lacking. So for this reason recently my team at Google deep mind introduced a powerful new embedding to address this issue called tips and this stands for text image for training with spatial awareness. This is a paper that we'll present later this month at Iclear which is one of the main AI conferences. So I encourage you to take a look at that too. We have code and models publicly available for this model. And to the final point in the question about how embedding quality can impact performance. This can be very significant actually. So for many applications on retrieve augmented generation or rag just like Anutant was uh mentioning uh you know having NLM model uh uh by itself it would simply not be able to answer questions if the uh right documents are not found. So this is of course a little bit application dependent for example may depend on the size of the index you may be retrieving from or if the types of data that are being retrieved are similar to what the embedding model was trained on. So um I encourage you to carefully consider embedding choice as it could definitely be a bottleneck for AI systems today especially given that loss are already so powerful. Okay, thank you. Absolutely. And we'll be adding the links to the papers that you mentioned as well as congratulations on getting accepted for Iclear. That's super exciting. Um I and if folks are interested in reading more about uh about Andre and his team's work um that would be a great place to learn more. Howard, is there anything else that you would like to add? Uh thank you Paige and thank you Andre. Um just a couple I think Andre covered it um pretty well. Uh I just want to add a couple of uh more points. Uh one is uh there's this um technique called the matrica uh embedding um which is actually currently pretty pretty popular. The idea is that if you actually have an embedding space with let's say a dimension of a thousand um we're training uh put putting some training uh uh you know uh requirement here to train an embedding which uh uh require the um shorter length embedding. For instance, uh the first 200 um uh floats or the first, you know, 100 floats to be also in the in the same embedding space in that uh you can basically uh trim the embedding uh such that you can still perform similarity uh similarity uh measure uh and they're all valid and that way um you can um you can you know decide how how big of the embedding you want to store from one training um and then you can always Um well each dimension does require uh you know pre to be prespecified but this is a pretty powerful technique for uh being able to uh efficiently represented embedding in these type of uh vector store uh database uh scenario. Um the other thing I I do want to I would like to emphasize is the use of LM uh for data curation. uh this happened in uh you know many recent papers including the Gemini embedding paper. Uh ARMS are really powerful and we all know that and then uh it is you know crucial uh right now it provides this um you know um capability for filtering lowquality examples and determining relevant positive and negative passages uh for retrieval and generate really rich synthetic data set. Um and then uh one last thing I want to mention is that uh you right now u some of the papers like EV5 uh they actually use um multimodel uh ARMS uh to bridge the gap uh for input. So this is mostly um used for multimodel embedding. Um and I think um you will see that trend uh become more uh relevant u and we in in in Google uh we're also working on something like that and then I think you will see um um Gemini embedding uh to become multimodel soon. Yeah. Awesome. Thank you both for pushing the boundaries of what's possible with embeddings and making sure that we can get these things out into the world so others can make use of them. not just not just Google and not just deep mind. This is really really cool and one of the um one of the most powerful ways to get research out into the world. Um next question um for Allen uh so when integrating vector search capabilities into existing database systems like Alloy DB versus using dedicated vector databases what are the primary architectural trade-offs regarding performance? So things like latency and throughput, cost, data consistency, and ease of use for developers who are building applications um like rag or semantic search. And also for folks in the audience who might not have heard of Alloy DB before, um if you could just give like a couple of sentence description of what alloy DB is uh and why it's useful. Sure. Thanks, Paige. And thank you for the question. Um so my name is Alan Lee and I'm part of Google uh cloud databases and the lead for alloyb semantic search. Uh so alloyb is uh one of GCP databases um database offering. uh it uh it is uh fully um Postgress compatible uh with additional innovations on the architectural side to make it uh faster than what is available for open-source uh Postgress and uh this uh allows our customers to uh be able to take uh their operational database workloads and um and run it faster on AODB. And uh we've incorporated um vector um search capabilities in it uh as uh additional um uh advancements uh for our offerings for customers. And so um going back to your question um so while performance is listed uh first in the question and is often the top of mind metric that u many think about I do see more and more non-performance requirements from talking with various customers and part of the shift is uh coming from the improvements in vector search uh performance of existing database systems that uh will generally work well for 90% of the use cases. And so um as an example, Alloy DB um it's uh we've incorporated um the scan algorithm as a native vector index type and um as uh the uh course participants uh may know from the white paper uh scan is based on 12 plus years of Google research and is our state-of-the-art vector search algorithm that's used internally at Google scale for products like uh Google search, YouTube ads and um the this algorithm has given alloyb the vector search performance that is more performant and scalable than what's available in open source and it's also competitive with specialized vector uh uh databases uh out there and um and so other important architectural consideration that um uh we should all be thinking about when um building on top of vector search is where the other data like what other data needs to be accessed for the overall application and where does that uh data come from. So for example, filter search is a very common use case for many customers and filter search combines a vector search query. For example, shirts that look like this uh image embedding and filtering conditions um size equals small and color equals purple and price less than $50. Where where uh this do these other filtering condition data uh reside? Is it um structured data that's already in an operational database or is it uh in a data lakehouse like BigQuery? So I if that's the case uh it may be easier to leverage uh the vector search capability of those systems and uh that will allow the uh user to express their vector search along with filters and joins and uh in a single system and in a single interface like SQL. Um otherwise uh there will be additional operational overhead and costs of things like ETL pipelines and implications of dealing with data consistency issues across systems and application complexity of having to join across multiple systems and uh that's without the help of a query optimizer that's typically there for SQLbased systems and these things make it harder for developers um specialized vector database like our vector search uh vertex vector search uh have their place as well. uh if data is largely unstructured like video, images and documents that don't already typically reside in a database or a data warehouse then you know none of the data consistency or application uh complexities that I mentioned apply and specialized uh vector databases can work quite well in these cases and um unlike databases and uh warehouses um that have to generalize across various workloads including nonvector workloads. Uh specialized systems can squeeze out the last bits of performance and that may matter for the 10% use case. And uh usually this comes with trade-offs like higher costs for purely serving out of memory or having looser data consistency models for index writes and updates. And lastly, depending on the use case, some other important considerations include security, compliance, reliability mechanisms like backup uh HA features. Just generally speaking, existing databases uh have more mature enterprisegrade features uh than specialized systems and that's mostly as a function of how long they've been around. So um u so it's important for the practitioners to think about like all the requirements of their entire application. That makes a lot of sense and I love that we've been able to take kind of this decade plus investment in research at Google and get it out into the world into the hands of customers through alloyb. Um that's I I'm hoping to uh to learn more about some of the use cases um at the Google Cloud Next as well. I I know that many customers have have been implementing Alloy DB in production and then uh getting serious gains from it. Um so thank you for the great uh thank you for the great answer. Next question. Um for Patricia and I I believe also also Alan might have some some thoughts around this too. Um, from a strategic viewpoint, how do you see the rapidly evolving landscape of vector embeddings and databases impacting enterprise data architectures and the types of AIdriven applications that businesses can build? What are the key challenges? So, things like cost management, standardization, talent gaps um that organizations face in adopting these technologies at scale. And in your in your role in the office of the CTO, I'm sure you must see this for many enterprise customers. Yes. Fantastic. Um uh thank you for the question because this is a very important one. I think that traditionally we have often underestimated the depth and breadth of the impact that certain technologies may cause. And I think that we are in the middle. I don't like to call it a paradigm shift but to offer or introduce an alternative where we are going from optimizing the indexing and retrieval capabilities of our data architecture to an exact match like in SQL across several fields of your structure data corus to being able to create several indexes of your data across multiple dimensions to optimize how you index on a per task basis. You can create multiple indexes per database and to search and retrieve data based on some fit for purpose metrics such as semantic similarity for search for example or proximity uh for certain attributes in in recommendation systems. And all of this in multimodel data where one can be looking for similarities but much more like nuances between video, audio streams, text, images, not just between concrete data points on the same modality as we are so used to do with SQL today. And this requires a paradigm shift across several dimensions. So if you look from an architectural perspective, we have to really learn or relearn how to ingest, organize, manage, make accessible new modalities of data that we have not dealt with on a daily basis as we are doing now, especially SEO instructor data. This also includes learning the variety of indexing mechanisms especially the ones that Google cloud and Google offers available to the different storage types and also understand the trade-offs between accuracy performance and scalability which I know it was in the white paper that you read also uh from a development perspective it demands developers to have like a deeper understanding of the meaning of embeddings which is a non-trivial concept As we also learned uh in the last day and uh from an application experience perspective, we really to need to reimagine the user experience that can now be offered because you can retrieve such a complex set of uh multimodality data. Uh but it doesn't come without its challenges. uh you have cost the infrastructure costs associated with deploying and maintaining vector databases. Standardization as you saw there is a lack of universal industry standard for vector embeddings and database interfaces poses integration and interoperability challenges. I would say the talent gap. Uh but now we have another 250, 000 people that are familiar with uh the concept of embeddings and vector databases. There are issues of scalability of and performance because you're really handling massive data sets with billions of vectors while you want to maintain low query latency and a careful architecture design and selection of indexing techniques. And you add on top of that data governance, security and compliance that cannot be neglected and integration with existing systems. integrating vector databases with traditional rel relational databases and existing business applications which can be very very complex. However, we are very very very optimistic because uh multimodel data representation through their embeddings in a scalable performant and accurately retrieval manner we believe that is going to form the foundation of modern enterprises architecture. So, thank you for the question. Absolutely. I I know that um you know, increasingly things like video understanding, being able to to have um insight into all of these different modalities are super super important. And I love that you called out that uh thanks to all of the folks who are dialing in, we now have over a quarter of a million developers out into the world who who will be able to help with some of these mission critical tasks going forward. So, this is this is great to know. Um, next question. Um, this is for Chuck. Uh, beyond standard cosign similarity or uklidian distance, what more sophisticated techniques or indexing strategies? So things like incorporating metadata filtering um or hybrid search approaches, reranking um are proving the most effective for improving the relevance and accuracy of vector search results in real world applications. And how are we uh sort of taking all of these learnings and embedding them into platforms like Vertex AI um or or in Google Cloud. Great question. Um thanks for the question and thanks for all the questions on Discord. Like we'll try and get to them offline if yours didn't get answered here, but uh lots of great activities there. So thank you. Um, I'd say just to start off and frame that question, there's a lot there about, as you as you've seen in that question, there's a lot of knobs you can turn for embedding systems that can make a big difference, especially when you're doing something at scale, but they might not be necessary for your use case. And so, you know, you heard Alan talk about the 90% use case, the 10% use case, 1% use case. And so, you're anybody who's talked to me is going to hear me talk about the importance of like a a good evaluation suite. So if you have an example benchmark data set or you know something that can help you figure out whether or not it's going to make a difference for your use case to add that extra complexity. Um in practice what we see people doing or being successful with um out of the things you were talking about I would say rag search rag hybrid search is one that really makes a big difference. So for people aren't familiar this is the idea of taking what we're talking about here with the embeddings those semantic embeddings usually um and then combining them with traditional keyword matching. So you want to be able to get it when somebody's when you have something that isn't exactly the same but is related. So a synonym you want to be able to pick those up with embeddings. It really helps makes a big difference. But those traditional keyword systems where if someone actually knows the title of the article they're looking for or the ID of the part that they want to find or the product, you really want to support that too. And so combining those two things and what people call hybrid search really helps. I think it's going to get more complicated um as you go on because we have people like Howard and um we have people like Howard and Andre who are doing amazing work with multimodal and so now hybrid search is going to start to include things like video and uh images and other things. So it will get a little bit more hybridy. So just be aware for that. Um and we you can see that in our tools. So like at Google Cloud and Vertex AI vector search we support both sparse which would be those keyword embeddings or keywords and then semantic in the same in the same tool. But you can also just pick up uh like BM25 in popular Python package and you can just try that out. So like if you look at code snippet zero in the white or code snippet one in the white paper, you can just drop in BM25 and do the same thing and get the same measurements for precision and recall and get a feel for how that would work for you. Um when you start to add additional uh additional modalities, so like if you're going to do keywords or you're going to do images, then you're going to fall into you're going to probably want to do some kind of re-ranking at the end. you may want to do re-ranking anyways, but it's really re-ranking is really helpful for finalizing that customer, that final list to the per to the customer or person who's going to be reading it. Um, and so, you know, some maybe you want to like combine a few different retrieval methods. You could just look at the ranks, but or maybe you want to do something where you're actually going to rank certain documents higher. So, like an official frequently asked questions document might be more reliable than something that you a document that you pull off of like the community notes. And so you'll see people do that and it can be really helpful for those sorts of use cases. Um just keep in mind your latency budget whenever you're doing that sort of stuff. Some of the other things that were in the question like I would sort of qualify as ease of use and really make a big difference at scale. So um you know filtering something can really cut down on latency if you can get rid of most of the relevant answers relevant documents early in your process. Um the same thing I would say for streaming inserts if you don't want to like rebuild your index all the time. That can be really helpful for your team. I think anytime you're picking up a new vector database, you really want to make sure that that it's going to meet your your company and your organization's ability to support it and maintain it. Like is it going to is it going to be easy to use? Is it going to be easy to monitor it? Does it meet your privacy and security requirements? So it's, you know, it is a new piece of infrastructure and you just want to be in sync with your other stakeholders um and make sure it's something that your group can can own and maintain and scale because I, you know, like like people were saying it, this really is a key uh piece of infrastructure at Google and I think it probably will be at your at your organization or in your projects as well going forward. So I think that covers it all. There are a lot of knobs to turn uh answers. Please feel free to ask more questions or follow-ups on the on the Discord and we'll be hanging around there to try and answer things there, time zones permitting. Thank you. Awesome. I love that answer and I love how you're making it real for people who might be hoping to implement some of these things in production. Things like filtering and re-ranking are are kind of hard-learned lessons and uh and optimization approaches internally at Google. But I I think for folks who are just starting on their journey, those are um those are good things to call out and to have top of mind on day one. So, love that answer. Um next question also for Chuck um for for embedding models. This is from uh from our discord cyclone um cyclone duster uh which I which I wonder is like cyclone uh cyclone destroyer. Uh I'm not sure how to pronounce it, but but I'm going to try my best. For embedding models, how do I upgrade to the newer embedding models um when they're available? Do we need to run all of the data in the database again through the new embedding models or is there a better way to do it? Um, so it sounds like this person is just getting started with embeddings and they're they're trying to understand all of these different models that are available. Um, how might they how might they use them effectively? So, I hate to be the bearer of bad news, but you do have to upgrade all of your embeddings that you've done when you switch to a new model. Um, and that's that's one of those things. Embedding models just aren't compatible with each other. And so that's like that's just sort of the the way the world works. I I apologize but it is like different languages or uh you know different versions trying to link to a to a different binary library that was compiled with something else try you know um so that that is the way it is. I think you know the the thing I'm going to say again is just the importance of good evaluations. So when you're doing that upgrade you want to be able to know that you have the matching version for the for whatever query you're using and whatever the index is that you're querying. And so having a good evaluation suite really helps that. And I would say in addition to the usual precision and recall and sort of like AI based metrics that give you a sense of the quality of those uh of what you're retrieving, don't forget to be measuring things like latency and other like important system metrics like you know the throughput, the latency, how does it perform under load because you will see with these more sophisticated models they do have a lot more parameters and so maybe they need a little bit more hardware. Um, but I think and there's nothing like qualitative feedback from your users, but but having a good like even a not so good evaluation suite can really help smooth that process and make sure that that everything is working um for you and your users as you as you change these embedding models. But yes, you do have to regenerate the embedding model, the re the embeddings for your index if you upgrade the embedding model. Um, and I would say the way things are moving, you've got really smart people at DeepMind and and in the community doing amazing work and you're going to want to be able to pick up those changes because it really they really are big improvements. And so I would say just be ready for uh for iteration. It's a it's the train is moving quick. Enjoy it. Yep. And the I love that I love that quote. The train is moving very quickly. We just released a new embedding model on AI Studio just a few a few weeks ago. Um Howard, it also uh did you want to add something? Yes. Um and thank you Paige and thank you Chuck for uh you know um endorsing our work at Jeep Deep Mind. I just want to add a little bit one uh one thing that u we're thinking you know leave no one behind. So the train is leaving but we also try to get the customer uh you know get the uh the passengers on this train. Uh so this is active research area. Um, but I think what we're really looking at is um how do you think about the embeddings, the previous embeddings and the newer embeddings and is there a more efficient way to map from previous embedding to the newer embeddings. Um I I think in terms of ongoing research, we see fairly promising results. It's not deployed. It's um I think um probably um many people in the research community uh are looking at the same problem because it is a pain to upgrade the entire database to new embeddings especially when the new embeddings are coming every day. Right? So just want to add that this is an ongoing research area. We see very promising results and I hopefully we'll see that in uh in Google Cloud and many places soon. Awesome. That's great to hear and awesome to know that that folks in the research world are are kind of pioneering some of these approaches to make life easier for all of the developers out there. So uh appreciate uh appreciate your response to this question too. Next question. um in retrieval augmented generation. So these rag systems, what potential impact could arise in retrieval accuracy or the quality of LLM generated responses? Uh if different embedding models are used for documents and queries. Um so Shiaoi, do you want to do you want to answer this one? Yes. Uh so thanks Paige. Uh hi uh I'm from Vortexi. I'm the uh like technical manager on the Vortex embedding quality. So uh regarding to this question, it's very critical that we use the same embedding model for document embedding and the core embedding generation. Uh the reason behind that is the embedding models they transform complex data like text, image, audio, videos into a shared vector spaces. However, this specific mapping uh from like real objects to vector spaces are different model by model. So using the same embedding model in rag will ensure that both queries and documents are projected into the consistent embedding space. This is crucial because the distances between the shared spaces are then used to accurately reflect the semantic rel relationship between the queries and documents. So please try to use the same uh like queries and documents from the same embeddings uh when you build your own rag. Yeah. Excellent. Thank you for highlighting those best practices. And I I think as many people are just starting on their journeys. Uh those are the the kind of gold nuggets that make all the difference uh to make sure that they get started on the right uh on the right foot and in the right place. Awesome. Um next question. Um in a rag system, indexing is performed in advance on the application data set. So how does the system ensure that query results remain relevant to the current context in a dynamic or real-time application after deployment? I love this question about kind of system drift. Um, Patricia and Anat, do you wanna do you want to take a stab at the answer? Sure, maybe start um there. So, first of all, uh um yes, measuring drift to see how embeddings um the change over time is super critical and coming back to what Chuck mentioned, having the evaluation suit, but also uh to measure the drift in performance, but also general distribution drift is important. However, there's one more key point which I think um especially in the context of rags of how embeddings are used with retrieval augmented generation systems which can make this whole process more efficient is that embed the process of embedding as well as embedding entire databases also depends on the dimensionality of the embeddings. uh um so if if you have a larger embedding of say 2, 000 um um however we if we use if you consider the concept of metroska embeddings where we can trim the embedding down and also consider the fact that um um like uh like you can make the the whole process much more u uh like efficient because in uh in larger LLMs with increasing context windows you can retrieve a lot more than just the top 10 or top five documents. So you do not need to necessarily care so much about the performance of the top uh like hits FK the the retrieval performance in the top five tops items. You can afford to uh focus more on like uh recall rather than precision where a few false positives are okay with our increasing context windows. For example, Gemini's um I think 2 million one to two million context window depending on the model you use is uh allows us to use slightly less performant embeddings but allow the indexing process to be much more efficient because we can use embeddings of lower dimensionality. So um that's one aspect to it. I believe um Patricia and Alan had things to share on this. Absolutely. No, thank you uh for that An because this is really uh Anant brought up the issue of using the context window and the flexibility on the retrieval allowing for false positives that can actually give a little bit more of uh items to be uh retrieved or exposed to the user. uh on the other side uh of the equation you would say on a complimentary side of the equation what you are trying to do is effectively bridging the gap between static index and involving data as well. So another aspect is to say look the data has drifted that we indexed the world as we saw it some time back and now the world is different. There are items that have been added and items that have been removed and there are several strategies focused on updating the vector database and its index in near real time in response to data changes. So one technique is called incremental indexing and which allows for the addition of new data to the index without requiring a complete rebuild. It's like adding these information and reduces the latency and computational cost that is associated with keeping the index always coolant. There is always uh also um what is called HNSW which is the hierarchical navigable small world which is a popular index in structure that generally handle the insertion of new vectors more efficiently than threebased indexes like anoi. Um research continues to advance in this field and we should always be mindful and uh look at what is coming. One of them is called the advanced uh inverted file which achieves higher um updated throughput I would say uh by maintaining policies and local re recclustering. But this is not all because of vector databases they often employ inmemory buffers to handle new data and background processes as they merge these changes into the main index. So change data capture mechanisms actually provide an automated way to trigger updates to the vector database whenever changes occur in the source application data set. So we have to look into that as well. Continuous indexing and embedding generation can also uh consume significant computational resources. So we really need to be mindful of that. An alternative approach just to wrap up is the real time rag or no index rag. So you don't create the index at all. It completely bypasses the need for precomputed indexes by directly quering live data sources at the time of the user query. In this architecture, when a user actually submits a question, the system transforms the query into a format suitable for the sources systems native search API and uh uh submits this transform the query to the relevant data sources. Um, in any case, it's always important to actually achieve a tradeoff between cost, performance, um, accuracy, scalability, and freshness of the data. Yeah, I love that answer. And Ellen, do you want to add something in addition? Yeah, just to uh briefly to add on to Patricia's answer, I think an important part of uh operational databases is the transactional and strong consistency guarantees uh between reads and writes and that includes uh index updates. And um so uh the what uh you know the transactional semantics uh means is that inserts and updates that are successfully written or uh immediately visible for subsequent reads. And uh this helps developers avoid dealing with eventually consistency issues like having stale data that shows up uh uh that has to now be hidden from the user. Um and this is also important for some business use cases where it is important to like have visibility to the latest data. So again this goes back to you know considering the um the requirements of the entire application. Um and in addition to what uh Anant said uh about um you know the data drift issues um you know there is also innovative work on u uh automatically handling index uh distribution or drift issues u to make it easier to use for um for everyone. Awesome. Thank you for the great question, Pashant. I as a workaround for this, I I sometimes use the the Google search API feature within the the Gemini APIs. Um, so you can ground information on the latest Google search results, which sounds very similar to one of the things that Patricia was mentioning, um, of just using kind of a a search to query the the live data source and and to get the response back and to incorporate that into into a summary. Um so lots of tools in the toolkit to make sure that as you're building your outputs from uh the large language models um or building these systems which drive the outputs um you're you're capable of getting the latest and greatest information. Um next question from Andy G. How does the precision of a single multimodal model for image and text embeddings compare to using a separate single modal model? Um and this is for Andre and for Howard. Do you all wanna Do you all want to answer? Yeah. Uh do you want to start How hard or should I start? Uh please go ahead. Yeah. So um I think the question implies uh that you're considering to use uh for a single modality search, right? Either a model that actually can handle mult modalities for single modality search or actually use single modality search for single modality uh retrieval applications because of course one just to make sure we're all on the same page. One of the big advantages of having a multimodal model is that you can actually retrieve uh you know search query with text and retrieve an image for example if you're talking about image and text it's modalities and vice versa and you would not be able to that uh uh you would not be able to do that using single modality models right so there is already one advantage but the other thing is that it's not necessarily that single model models will necessarily be better in single model uh single modality evaluations because for example uh when you look at specifically at image and text uh there's a lot of uh data uh uh online for example that has text supervision text annotated together with the image and just by learning uh models that way that are already going to be multimodel you are actually able to leverage a lot of supervision which sometimes you're not able to do if you want to have uh manually rated data for uh uh images which is much more expensive. So um and the other thing to mention is of course many of these things depend uh on the model that uh uh sorry on the data that the models were trained. How large are the models? Generally we see the trend in the community of going to larger models more capable models that will do well across many different modalities uh rather than um having specialized model modality. Of course, you can also apply this technique called knowledge distillation to take a really big model, really capable one and distill on a smaller model to accomplish a task that is a little bit more restricted um or just to have a smaller model generally. So, um yeah, these are some thoughts. Um Howard, is there anything else you wanted to add? Yeah. Um thank you, Andre. And I I one thing that um that we observe as a trend is that um uh people are using or multimodelms um as a way to bridging the gap between modalities. So uh for instance take um the EV5 um model they they basically have the multimodel model uh LRM to as an you know a way to take the input from any modality and then uh what they they were training is p purely training on text right but because the uh M ml uh so the ARM has already taken care of the input modalities so it effectively once you know by that I mean they map images and text and into the same embedding space. So all you need to do now is to organize this embedding space based on text information. So let's say if you are clever well you can you can you know really organize the um word of airplane versus fighter jets versus you know aircraft carrier in the space uh you know relatively then images of air airplane fighter jets and aircraft carrier will be automatically organized because these are shared embedding space. So that technique is actually fairly powerful. Um I think you know what we observe is that today uh you know I I so a couple of the um representative models so you have single modality models uh what we call do encoders where you just align them for image text um uh tasks uh alignment tasks like uh clip or align and they used to be state-of-the-art but now these uh single multimodel models started catching up um examples are kokai and then EV E5V um they you know I I think the trend is that we'll probably see uh the single multimodel models uh will achieve higher and higher precision and close the gap and you know even surpassing the dual encoder models um that uses separate single model components. Awesome. Fascinating. I love those answers and thank you so much Auntie G for the great question. Um, and with that, I want to give a big virtual round of applause to all of our wonderful Q&A um, experts who came here today who gave their time to to be able to to share more about what they've learned as well as to help answer our community questions. We really appreciate all of you and your time. Um, and thank you uh, thank you so much again. Um, the next section is going to be all about our code labs um, around embeddings and vector databases. Um, thank you so much to Mark McDonald for creating the notebooks and uh, Anat, do you want to take it away to to explain them? Um, folks on the call, you can uh, you can also uh, kind of uh, uh, go off uh, uh, go off into the background while not doing the the explanation of the code labs. Thank you so much. Anant, you might be on mute. So, so, uh, yep. All right. Yeah. So, uh, just to be efficient, since we have, uh, we had an interesting conversation, I'm going to make this section super short. Uh, I'm sharing my screen now. Um, this notebook is, uh, rag uh, retrieval augmented generation. Here you will see how you can utilize the um the Gemini API and open source databases to build it a simple rag pipeline out. So firstly we see um in the first section of the notebook we see how we make some synthetic data three three paragraphs and then we in the sec part after that we see how we use chromma an open source inmemory database to kind of uh index so um to index this data and create um um create like so we you have a embedding uh we use embeddings um the to embed the text data and then persist it in the chroma DB. And after that we see uh we see that was the index sorry that was the generation embedding and indexing part. Then comes the the retrieval part of RA where we see um how we can leverage this um um database uh to query take take a natural language text a query from the user uh and um the database internally embeds it and uh retrieves um retrieves the top uh k most uh similar results and then after that we see okay now that we have received uh so we retrieved the um most similar results how can we use that to answer the question using the Gemini 2. 0 flash um EDLM and that's the last part of the collab where we do the generation part with the retrieved documents by the semantic search. So that was um that's your very basic u um RG app. Now I'll move on to the second collab. So moving on to the second collab. Um um yeah, give me a second. Um yes. So the second collab talks about um exploring the semantic aspects of embeddings to see how similarly or documents with similar meanings are grouped together in the shade semantic space. So in this collab we see how we define some sentences um some synthetic sentences and we we embed all of them using the Gecko embedding model by Google and we see how they look how their semantic similarities look in um through a heat map. So in this heat map you see that uh on a scale of one to zero where the darker shades represent values closer to one and the lighter ones represent closer to zero. We see that documents which are more or less meaning the same uh are have a darker like shade which means they have much more semantically similar. And of course the diagonal represents a document being compared to itself and a document will always be more sem most semantic similar to itself. of course, but then we see how others um uh other documents um are not very similar and you see lighter shades there. For instance, um the quick brown fox jumps over the lazy dog. This is very semantically similar to itself and a slightly misspelled version of itself. Uh however, it's not very similar to the five boxing wizards jumping quickly and so on because these mean two different things. So that was um the collab on semantic similarity. Let me quickly go over and walk through the last collab which you're going to see. So um we talked a lot about embeddings being used for semantic search approximate need to find similar documents etc. But there's another very important application of embeddings that is using an embeddings for as as a feature extraction of sort uh for downstream models. And in this collab we see exactly that we kind of use an open source data set the news groups text data set uh get the train and test uh splits for it uh pre-process the data set in the subsequent section to kind of make it nice where each of the text the the the news articles have various classes and we are basically taking the text and trying to classify it into the right um target class. Um so we see that um if we use the embedding models to create like uh embedding representation of the news text and then train a classifier on top to tune uh like I guess add a layer or two afterwards and only tune those two layers is very efficient since uh the whole the embeddings already carry very rich representations. We use the classification task type for the embedding models and uh we add um uh uh we use that to uh as in kas to basically um get the embedding representations uh for the text and then build a classification model with that and you see that we get uh pretty good performance um um um in the u uh in in the task after that and we can also try it on our own handwritten texts. So this is how you can use embeddings for um downstream models and beyond just semantic search. So that'll be all off to you Paige uh for the next section. Thank you so much Anant. And now we are going into the last and final section of today's uh of today's course. We're going to be doing the pop quiz. And so our first question, which application are embeddings not the best suited for? Um, is it retrieval augmented generation or rag? Is it simple rule-based systems? Is it anomaly detection? Or is it recommener systems? What application are embeddings not the best suited for? I'm going to count down. Everybody grab a piece of paper and a pencil and jot down your answer or just type them out um someplace handy. Um five 4 3 2 1 and the answer is B. These simple rule-based systems are not something that embeddings are best suited for. Um, next question. Which of the following is a major advantage of scan over other approximate nearest neighbor algorithms? Is it A, it's widely open- source and available? Um, B, that it's designed for highdimensional data and has excellent speed accuracy trade-offs? Is it C that scan only returns exact matches? or is it D that it's based on a simple hashing technique and has low computational overhead which is a major advantage of scan over other approximate nearest neighbor search algorithms. And I count down five 4 3 2 1 and the answer is B. It's designed for highdimensional data and has excellent speed accuracy trade-off. So hopefully you were paying attention to the uh to when Alan was talking all about alloy DB. um and some of the some of the scan research um approaches there. Question three, what are some of the major weaknesses of bag of words models for generating document embeddings? Is it A that they ignore word ordering and semantic meanings? B that they're computationally expensive and require large amounts of data. C that they cannot be used for semantic search or topic discovery or D that they're only effective for short documents and fail to capture long range dependencies. Um so so what is uh what is some of the major weaknesses of bag of words models? I'm going to count down five 4 3 2 1 and the answer is A. That they ignore word ordering and semantic meanings. Question number four. Which of the following is a common challenge when using embeddings for search and how can it be addressed? Is it that embeddings cannot handle large data sets so you have to use a smaller data set? that embeddings are always superior to traditional search, so there's no need to kind of couple together search. Um, is it that embeddings might not capture literal information very well, so you have to combine with full text search, or is it that embeddings change too frequently and you have to prevent updates from happening? Um, so what is a common challenge and how do you address it for embeddings? I'm going to count down. 5 4 3 2 1 Hopefully no one thought that the best approach was to prevent new information from uh from happening or new things to occur. The correct answer is C. That embeddings might not capture literal information very well and so you should combine them with full text search. Question number five, what is the primary advantage of using something like locality sensitive hashing for vector search? Is it that it A guarantees finding the exact nearest neighbors? B that it reduces the search space by grouping similar items into hash buckets, c the only method that works for highdimensional vectors, or D that it always provides the best trade-off between speed and accuracy. What is the primary advantage of LSH for vector search? I'm going to count down five 4 3 2 1. And the answer is B. It reduces the search space by grouping similar items into hash buckets. So, thank you so much to everyone. Hopefully, everybody got 100% correct on the pop quiz. Um, and we will see you tomorrow where we're going to be doing the next um instance of of learning for our Kaggle generative AI intensive course. Um, really jazzed to see what questions you might have over the code labs, the white papers, and the podcasts over the next 24 hours. Um, and see you all tomorrow. Thank you so much. Heat. Heat. Heat. Heat. N. Heat. Heat. N. Oh, it's down. Woo! Down down down down. Heat. Heat.