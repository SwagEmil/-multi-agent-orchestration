Title: DAY 3 Livestream - 5-Day Gen AI Intensive Course | Kaggle
Channel: Kaggle
URL: https://youtube.com/watch?v=g6MVIEzFTjY
Caption Type: auto-generated

======================================================================

## Cleaned Transcript

Greetings everyone. Welcome back to our third day of the Kaggle Generative AI intensive course which is all about generative AI agents. I'm Paige Bailey and I'm really excited to be here today to get to teach over a quarter of a million of you all around the world everything that you would ever need to know about generative AI and how you can incorporate it into your software engineering projects. This discussion is going to be extremely extremely cool. I'm very excited about some of the folks that we have on the call. and I can't wait to get started. So, just as a general reminder, course overview, this is a five-day intensive course taught between Google Cloud, Google Labs, Google DeepMind, kind of all of the all of the folks that we have working around, around Google in the AI space have come together to build this course. and we're really excited to have you here today. this is day three which is all about AI agents and hopefully you've been learning to build sophisticated AI agents over the last 24 hours. I want to thank our course moderators who have been tirelessly answering all of the questions on our discord as well as doing great things like sort of testing out some of the course materials over time doing all of the logistics and behind the scenes work in order to actually make the Kaggle generative AI intensive course happen. So, a big virtual round of applause for everybody who is devoting their nights, weekends, and and all of their energy and love into making this course a reality. So, thank you so much. and with that, I'm going to turn it over to Anant for a quick curriculum overview of the white papers, podcasts, and materials that you've been learning from over the last 24 hours. So, take it away, Anat. Thank you, Paige. Hi, everyone. so welcome to day three and you might have seen in the first couple of days that we learned about foundational models and how we can use prompt engineering along with day 2's embeddings and vector databases to provide the model with appropriate instruction and context for the job. So in today's white paper, we're diving deep into generative AI agents where we take what we have learned and show how LLMs can use the tech that we have discussed so far, build upon it and use them dynamically. yeah, so let's recap the main ideas of the white paper. So first we read in the so we have you have two white papers, right? One is the first one being the introduction to agents and the second one being slightly more advanced. And in the first one we read what a that agents are basically AI applications that use models to achieve goals. Unlike standard ones they observe, reason and act using tools and operating autonomously to or semi-autonomously to finish the objective that they were assigned. Then we also saw the their core components for example the model for instance Gemini for reasoning tools which include extensions functions and data stores for interacting with the external world and the orchestration layer which manage manages the thinking and action cycle using frameworks like react or chain of thought and more. Now then this was followed by the section looking closer at tools. Extensions which is one of the tools let agents directly call APIs. Functions involve the model suggesting calls for client side execution giving developers like yourself more control. And then finally data stores which ground access to external data often via a rag or to keep responses grounded. The papers also mentioned that as in enhancing agent performance through techniques like in context learning or fine-tuning but you'd find more about that in the day one's white paper. So in the second white paper which dives a bit more advanced there were a lot of topics which which were covered but what were the key points are things like agent ops and rigorous evaluation for agents. agent ops talks about deploying the important practices for deploying agents in production. You'll be seeing more about that on day five. And but evaluation talks agent evaluation talks about tracking metrics like goal completion, analyzing the agents action trajectory, assessing the final response and including human feedback where necessary. And we also have a service around this on Vort. ex which might be of interest to you. and then in the second white paper we also covered multi- aent systems which has been quite popular recently where specialized agents collaborate on complex tasks using patterns for example like hierarchical or collaborative setups to achieve the objectives. Agentic rag where agents define the retrieval process is a one of the keyh examples of where multi- aent systems could be very useful. The white papers concluded by discussing agents in enterprise as assistants or for automation managed via platforms. For instance, Vert. x AI's agent space and also an interesting new concept which is coming up which is defining contracts for increasing the reliability of agents. We also looked at some applications like co-scientist of multi- aent setup as well. So that's the essence. now we'll explore more in the code labs later and the Q&A session which follows. So off to you, Paige. Take it over. Awesome. And excellent. let's move into our Q&A with all of the experts who have who have come online today to to be able to talk about the products that they're building and deploying all around Google Cloud as well as in Google Labs and Google Deep Mind. So I am overjoyed today to welcome Jacqueline, Antonio, Steven, Julia, Alan, and Patrick onto the call. we have a lot of questions coming in from the audience. So, thank you for adding all of the all of the great questions on Discord. and with that, I I am going to go ahead and get started. So, first question. welcome Stephen and Jacqueline. I am a really devoted and excited user for both Notebook and Project Mariner. it's been really exciting to see all of the all of the great work that y'all have been doing to ship agents into production. so let's let's learn a little bit more. Stephen, could you tell us a little bit about Notebook LM? Yeah, sure. so, some of you may have been experiencing, Notebook LM through some of the, audio overview podcasts you've been listening to as part of the programming here. we think of of Notebook LM as a as a tool for understanding things. so you give Notebook the documents that you're working with that are important to whatever project you're on. And those can be docs or PDFs or slides or audio or YouTube links and number of other formats. And once those documents have loaded into notebook, it's like you're talking to an AI that's an expert in the information that that matters most to you, that it's important to your project. And that means it's it's personalized. It's grounded in the in the facts in those documents. we have a really advanced citation system so that when the model answers a question based on your documents, you can see the citation. You can see the original passage that it used to answer the question. You can click and read the original source material. So, you can fact check and and confirm that the answers are are viable ones. And we've been adding tools to transform the information that you've uploaded into new formats. so audio overviews is kind of the most famous format we have. So, take your sources and suddenly you have a 12minute podcast conversation between two amazingly lifelike AI hosts. you can create a briefing doc or a study guide and text. but we're actually adding we've just added a new feature that's been getting great reception. We're really excited about it. I'm going to share it on on my screen here so you can see. it's a feature called mind maps. And basically it will take your sources and turn it into a a kind of visual representation of the concepts in whatever sources you've uploaded. so you can have you know your your in this case this is a American history textbook that I've uploaded and it's broken it up into all these categories but you could put your personal journals in here. You could put your codebase in here like you will get a a really interesting visual representation and you can see these are the categories subcategories. You can kind of like zoom into one of them and see you know kind of further topics. And what's so powerful about mind maps and notebook LM is it's not just a a picture of the concepts in your documents. Every single one of these nodes in the mind map is actually a clickable query that you can then send to the model in notebook and you will get a a detailed overview of that particular topic with citations all the all the amazing kind of tricks that notebook has up its sleeve. So it's a really genuinely like new way to navigate through complex information and it it's now live in all notebookm for all notebook users. This is so cool and I really wish I would have had something like this when going through university or just getting started with some of the code bases in Google 3. I I think this is really really amazing at reducing the complexity for these really really kind of intimidating systems. and it's great to know that that folks can test it out today. and speaking of navigating complex systems and doing useful things, Jacqueline, tell us a little bit about project mariner. Yeah, absolutely. So, Project Mariner is a research prototype that's really exploring the future of human in agent interaction starting with your browser. And what that means is that you can ask Mariner to complete a task for you and it will actually navigate the internet. It will click through different websites. It knows how to scroll, how to type, and it can actually get things done on your behalf. So, we launched it back in December as a Chrome extension and it's been really interesting to just see what our trusted testers have been up to and watching Mariner think through the steps it should take to complete these complex tasks on behalf of the user and actually get them through to the end of of accomplishing the things that they need done. Excellent. I I have loved seeing all of the great demos for Project Mariner and I I also love that it's it's just a simple Chrome extension that you can just add to Google Chrome. and you're off to the races. That is very very cool. Love the the focus on usability as well. And I think we'll be hearing a little bit more about Mariner's agentlike approach and some of the questions coming up soon. so everybody should stay tuned. next question. This is for you Stephen. agents hold the promise of being collaborators or tools for thought, which is I know a phrase that you you have used and and throughout many of your books and throughout much of your research. So, drawing on your experience with projects like notebookm or other other projects that you're working on in labs, what design principles are the most important for creating AI agents that successfully augment human capabilities rather than just automating simple tasks? And how do you balance agent autonomy with user control? Yeah, balance is is such an important word here and it's a great question. I could talk about it for the rest of our time here actually. So you know one of the core principles at notebook is the idea that the the model is always grounded in the sources that you've given it. Right? So that that's a defining thing that we we did from the beginning. so that you put in documents the model will answer based on the facts in those documents that it has been specifically instructed to stick to those facts. So if you upload that American history textbook and you ask a question about Taylor Swift, it will politely decline to answer the question even though the model knows a lot about Taylor Swift, right? And so that focus is a huge part of the value proposition that that notebook presents. However, we want people to use notebook in a in a creative way, in a brainstorming way. We want notebook to help them think things that they wouldn't have otherwise thought about the documents that they have in front of them. And so we get a lot of queries which are like hey based on the notes that I've uploaded here what's a adjacent related idea that I haven't thought of yet. And we want the model to be able to do that but sometimes you know with our source grounding kind of instructions the model will come back and say like I cannot tell you about some other new idea because it is not mentioned in the sources. And so we've spent a lot of time crafting the instructions to say listen you know the the basic framework should be these sources but you are allowed some creative latitude to explore kind of related ideas just don't go too far a field and it's it's kind of like building a a leash on the model where you can give it a little bit of room to move and navigate around the core kind of ground truth of the sources. But the one thing I would recommend for folks who are interested in this kind of work is every version of the model handles this kind of task slightly differently we found. So we would get our system prompt to you know just the right amount of freedom and then we'd get we'd move on to a new version of Gemini. We'd be really excited because there were all these capabilities but it would be maybe more faithful to the initial instructions and wouldn't would kind of decline to do those extra creative work and so we would have to rewrite the prompt. so every every generation of the models have slightly different interpretations of what its instructions really are. And that's that's part of the art and maybe not the science of of this kind of work right now. Yeah, I love the focus on collaboration in in notebook and being able to to kind of have a conversation with all of your source material. I I think that's a a great path for folks who are who are looking to to kind of incorporate more AI into their daily into their daily work. and also to learn a little bit and build intuition about how to interact with these systems. So, it's it's been very very cool to see. Thanks, Paige. Yep. Excellent. Next question for Patrick. defining, evaluating, and debugging the behavior of autonomous or semiautonomous agents is super complex. And I know this is something that a lot of our customers on Google Cloud have been trying to think through over the last several months. What new methodologies, testing frameworks, or observability tools are being developed or are needed to ensure that generative AI agents perform as intended, remain aligned with user goals, and can be reliably managed once deployed? Yeah, that's a that's a lot there, but yeah, I mean, I think evaluations are probably like one of the most important things that you can do when you're deploying your agents into production. and so you'll see a lot of that material covered in the companion paper on agents and you'll kind of hear it throughout the coursework as well, just kind of the reiterating how important evaluations are to kind of making sure that your production agents are doing what they're supposed to be doing. Now, historically, you know, we've kind of had this concept of like the golden evaluation data set, right? Where you have to manually define what are the requests going to look like and what are the responses going to look like. you expect a certain number of tool calls, they need to have happen in a certain order, right? and so defining this golden eval set and then testing it against your agent can actually be really brittle. So while it might work right now, as Stephen just pointed out, as soon as you get Gemini 2. 5 or Gemini 2. 6 or 2. 7, whatever the next model is, it's just like all of those tests just kind of fall down. And you can sort of think of this as kind of like you know unit tests is like if you're fundamentally changing you know the core of your your program all your unit tests are going to fail you have to rewrite your unit test right and so there's just kind of this back and forth back and forth that's going on with evaluations and while evaluations are important we realize it's a lot of toil for users to have to manage that with the pace of the new models coming out and things like that. So one of the new innovations that we've been working on is this concept of testing scenarios instead of like you know strict golden data sets and the concept of the of a scenario is really more abstract versus just like an actual strict thing to follow. So you know an easy way to think about this is imagine that you're building like a retail agent that does order status checking or something like this. In the traditional sense, you might say, you know, I need to check my order status. You know, what's your order number, tool call, tool response, etc., etc. In a scenario-based environment, you're basically just explaining to another AI agent that's going to be running the evaluations that you're saying like in general, as you look at how this conversation has progressed, we want to make sure that this task was complete. That task being that an order status was checked. We don't particularly care about which tools were called in what order. We don't care about any small talk that happens in the middle. We just care that the task itself was, you know, pretty much, you know, completed, right? And so that's really like the fundamental concept around scenarios. It's much more abstract. It's easier for sort of the average person to write a scenario than it is to write a golden evaluation data set. And then you get a lot more robustness out of your testing. So even as the models change, you still say you still have the the abstract concept that can apply to any of those variations of models that increase over time. And a lot of this also allows you to do kind of like full autonomous testing, you know, constantly as new models are being released. Yeah, I love that the the focus on outcomes for evals as opposed to trying to explicitly write out a recipe for each one. I think that that makes the process of writing evaluations even easier than than some of the frameworks that we've already seen created and it also means that folks even outside of software engineering like in the product realm or or project managers can even contribute to writing evals. So this is this is very very cool to hear. next question. for for Patrick and for Julia, building robust AI agents requires effective tools use and grounding and reliable information. So, how are capabilities like function calling and grounded search within Vert. Ex AI designed to work together to enable agents to execute on these kind of multi-step tasks accurately and safely and reliably. I'll let Julia kick it off, I guess. Sure. so the way that we see these things is these two capabilities really kind of work together and form the basis of a lot of the various agents that we see functioning today. Think about concepts like deep research, the co-scientist, etc. Essentially, we're allowing the agent to first retrieve some of that necessary information and then plan a sequence of actions and then execute on those actions based on on on function calls. And while we spend a lot of time, you know, thinking about all of these different pieces, I think one of the key things that u Stephen actually touched on earlier is kind of that prompt and that is essential in the function calling side. We spend most of our time actually debugging functions with customers because that's where often the the issues lie. So really spending a lot of time tweaking and adjusting those kind of components and pieces and ensuring that that really is the end task that you're trying to solve is probably the most critical component to to to making those things work. And I think this ties into the question that Patrick actually just answered as well that these scenarios and as like we like to call them oftentimes simulation tests will actually become more and more important. And I had a researcher once describe it to me as up until now we were looking at multiple choice questions. There was one right answer and now we're looking at an essay question where there are many different kind of right ways to get to the to kind of the scenario to to answer the question correctly. And I think that's a really good way of of thinking about things. And that's kind of the direction where we're heading in across the board of of both evaluations and w with function calling. And that is the balance that you kind of have to strike. Now, functions need to be defined very crisply to execute on the task that you're trying to to solve to answer that specific essay question. I love how how clear and how beautiful that analogy is and I'm I'm going to borrow it to like thank you for thank you for sharing. Is there anything you want to add? Yep. Yeah, just a quick addition. I mean, and I love that analogy as well. It's like one of my new favorite analogies to use. Thank you, Julia. yeah, just another comment on on function calling. You know, one of the interesting things that we've seen from like a research research perspective, as you know, you spend a lot of time putting your prompts together and trying to get your tools to get called and function calling perspective, we see that like the more that you can actually get your your tools called or function calling happen happening during a multi-turn conversation, especially a multi- aent conversation, the fewer hallucinations that you will have and like the better and more grounded that that kind of conversation will be. So you can imagine if you're just having sort of like a vanilla conversation without any tool calls, the longer that conversation goes on, the more propensity there is for hallucinations to start to occur because there's no any kind of tool calling. You're just relying solely on model knowledge. but let's say if you're in a business environment, an enterprise environment, you really need to make sure that there are no hallucinations. that means you need to start kind of trending towards more tool calls to make sure that you're grounded in you know whether it's Google search grounding or just you know API calls database calls behind the scenes so it's really important to have those tool calls and then one of the key points I think both Stephen and Julia called out is around the prompting aspect inside of prompting you know we have this concept called fshot examples and I think it's really important for people to understand this concept of fshot Gemini really works awesomely out of the box. But if you add few shot examples on top of that with tool calls, you can really build a a really nice hardened system to ensure that your tools will be called in a specific order with lots of different, you know, parameters that are always going to happen the way that you want them to happen. So I would say like prompting few shot examples and tool calling, they all kind of go together hand inand to build a really nice production grade system. Yeah. And just even as a person can get a much better output or much better response if you show them a few examples first. Large language models are the exact same way. So as you're experimenting with Gemini and AI Studio and elsewhere like make sure to include those few shot examples, those those multiple examples of of what good looks like so the model can understand. Yeah, I believe Jacqueline had something to add. Yeah, I was going to say just one other thing I've seen come up on this topic in eval especially as we think about moving towards essays not just multiple choice is defining what's good and that's becomes a lot fuzzier but it's really worth spending a lot of time to understand and like get in agreement with your team on like what is a good output and how are you grading those responses and I know Stephen firsthand that on the notebook team that's something that you've spent a lot of time is like how do you define what the output of a good audio overview is that's very different than just a right or wrong answer and that's something that's new in this era of AI I agents and these like more fuzzier outputs that we're now trying to evaluate against. Can I actually I have a funny story about this. So we added to audio overviews a few months ago this ability to join the conversation. I think people have seen this as interactive mode. so you can be listening to the podcast but you can hit join and then you can ask a question and the host will then dynamically change the content of the podcast based on your question. It's really amazing. But when we first did it, the when we rolled it out, we found that the hosts were strangely irritated by being interrupted. And so they had this whole attitude of like, well, fine. I mean, we were going to cover that later, but I suppose and we literally had to do, we talk about prompt, we had to do what we called friendliness tuning to make the models to like to make the host just a little bit less like nasty to the to the user. And we were all just like, I can't believe this is our job. what is what is even happening? But from a user perspective, that attention to detail matters so much and it comes in clear in product truth for for all of the folks that are using Notebook LM and using the tools that that the folks on the call work on. So, thank you for for having that attention to detail and doing the hard work to make sure that the experience lands in the way that that is most optimal for for human results. Excellent. so next question, this is for Antonio. as generative AI agents move from experimental stages, so some of the the tools like Project Mariner and Notebook LM, which we've just been learning about, towards broader deployment on platforms like GCP, what are the biggest hurdles for enterprise adoption? Are they technical? Are they ethical? Or are they related to managing these complex agent behaviors and integrations? And Antonio, you might be on mute. I'm still smiling because I think that you just invented a term which is friendless tuning. Still, that's what we do, right? That's that's exactly what we do. and so I think that you already answered this this question because at the end of the day, it's all about usability, right? So what how we satisfied user needs. So you are on to something, Stephen here. Well seriously I think that one one aspect that we are seeing right now is that more and more pipelines are agent pipelines are the combination of multiple models and these are is becoming more more and more popular. Until very recently you were seeing probably one large model with a bunch of prompts. Now you're seeing multiple agents that are collaborating each other and to to evaluate and to put this agents in in in production you basically are moving from singular to plural right so instead of having one single large model in production you have multiple agents that are collaborating so plurality of agents and is essential to put in production to have to define what what Jacqueline was saying like what's the definition of good the definition of a good here is a local definition of good for every single agent who is performing a specific action and also a global definition of good. what how the end to end pipeline is is is behaving. And so you need to have multiple metrics, multiple definition of of goods for for taking into account accuracy, efficiency, bias prevention and also alignment, right? And and the topic is becoming more and more complex. We have tools for doing this because of this plurality of of interactions. Awesome answer. I I think the there are a lot of folks who are in enterprise companies today trying to put agents into production. so thank you for helping them kind of think through some of the some of the top of- mind items that they need to consider when deploying those agents. next question also for Antonio. how can we ensure that long-running agents stay aligned with their original goal and avoid drift especially given the complexity and tokenheavy nature of LLM driven reasoning and tools use. Well, this is I think this is a very fundamental question. We are used to have for pipelines in in different sectors we are using to have like things like CI/CD. before Patrick was mentioning the parallel here is the unit testing right and I think that what you need to have is to have ways to observe how the pipelines are working evolving over time essentially connecting to what I was saying before if you think that you have multiple agents that are collaborating you need to have metric for this agents and magic for the wool pipeline itself tools that we are seeing used more and more have LLM as a judge for sure this is actually helping a lot having an external large model that is judging the behavior of every single agent and the bull pipeline itself we are seeing more and more usage of reinforcement learning very frequently paired with humans if these pipelines are actually supporting humans a fundamental question is what's the behavior of the pipeline when they are augmenting humans and what will have been the behavior of humans in this particular situation so reinforce learning is used with human evaluation, human feedback but also more and more we are seeing things like in situations where you can reproduce an outcome. So you have this thing is called rewards that are indeed some rewards that you can verify. So code execution as well as situations where you have you can run an experiment and have a quantitive evaluation. In all this situation, reinforcement learning is working well because you can have a program programmatic improvement of things, right? and we are also seeing a usage of data that is evolving over time for doing back testing and so trying to fine-tune the agents for what will be the their behavior in the future where the future is already the past because you have this data used for for back testing. This is a very complex topic but there are if you go into the white paper and the companion white paper you see many examples of this. Yeah and we'll be covering agent tops even more on day number five. So thank you. Thank you so much Daring Beagle 34506 for our very first community question of the day. Wonderful. Next question for Alan. does Gemini support MCP? so so I know this has been top of mind as an open standard for many many folks around the community. and I've personally really really enjoyed working working with MCP. If so, how can we integrate MCP with Gemini in an agent-driven flow and what are the benefits or drawbacks that this combination might offer? totally the MCP standard has been growing in popularity a bunch especially in the last few weeks. It's it's just like hockey sticking in popularity. but it's been around for several months and and we've been playing with it for a while. we have some examples right now in Gemini examples repo and generative AI repo. We've got several public things. I'm sure we'll get some links to you. I think of MCP as just kind of like a thin shim around an API or like a universal SDK. It's actually kind of fun to I've spent 20 years wiring APIs together and it's really fun to just like pop in servers from different companies and see them automatically working without a lot of wiring. The discovery it is really convenient, right? that convenience is going to continue, I think, to make it very popular. and any framework that's out there for building agents is going to support it if they don't already support it. and then the question of wrapping APIs into an MCP server and running that MCP server, that's still kind of an open question in some ways. you can do it pretty easily. And then the question is, does it have all of the parts and pieces that you need to really run this in production? we are working with Anthropic on trying to push forward what we would like to see in this standard. there were announcements, I think it was last week or the week before where OOTH was announced and some other security additions. but you should absolutely be playing with this. I'm maybe a little old school and I'm not sure I would deploy it for a very sensitive production use case depending on your needs and how you deploy it today, but it will absolutely be there. and it's it's coming along. So, yeah, sorry long-winded answer. We support it. We will probably continue to support it in lots of new and interesting ways. keep looking. Excellent. And I love that it sounds like we're collaborating closely with Anthropic and exploring it for for our own products. This is this is great news. And if folks haven't already been experimenting with MCP, make sure to check out those examples on GitHub as well as to check out some of these large hosted collections of MCP servers like Smithery or Composeio. cool. So, next question, another community question. how do AI systems assess the reliability of real-time data from multiple sources, resolve contradictions, and how can they recognize and correct their own incorrect decisions? this is another community question from Johnv. and Julia and Jacqueline, do you want to do you want to give some insights based on the products that you're working on? Sure. I'm happy to take a a first quick pass here. so from our perspective or the way that I kind of think about this is that there's two pieces to this and we've touched on both of them already. There's the evaluation side. So as you're kind of testing and putting the agent out and then there is the kind of real time piece to to this question. I think we've talked about the evaluation scenario testing simulation and various other questions. So I won't go into too much more detail on that. But I think where it becomes interesting is in that kind of real-time structure. And what we have seen quite a bit there is another agent that is specifically tuned to kind of collect all of the information, evaluate it and merge it together into a response or to kind of decide whether or not this is the response that we kind of provide back to the end user. So, I think that that's kind of what we're going to start to see. And to kind of use another analogy there, you kind of have this team of folks that you are are are setting out to operate that are, you know, producing a specific response and then there is kind of a manager or someone that kind of makes that decision as to which response is then ultimately provided. And I think that that's what what's what we're going to start seeing in in real time. And that's where we tie back then also to the evaluation side because collecting those different examples, figuring out where you know that evaluation agent was potentially wrong is going to be key to kind of continuing to then improve the system as as these different components work together. And you can think about it much as like a team that's working of humans that are also working together in in in some of that sense. So you kind of, you know, move forward, you you you put some things out there, you gather feedback, and then you collect and and and adjust as as you continue to move forward. Yeah. I think just like adding on to that, the way I also like to think about it is either like a team of agents or just multiple steps in your AI system that you're building where maybe the first step is you generate, you know, five candidate responses and then you can pass it to another prompt which says evaluate which of these five prompts are or five responses are the best and select the best one. Then you can move on to the next step which is okay really reread this response. Is it accurate? Is it fun? Is it like is it good? However you've defined what good is. If so great that can be your output. If not okay let's go back to the beginning. And this role of like a critique and a critique loop becomes really important so that you can always go back and start again even in these real-time scenarios. It does add latency. So these are things to consider as well and cost but it's certainly like having these loops that can go back and self-correct even in real time become incredibly important. and the way you prompt them also becomes incredibly important. So a lot of just you know prompt engineering, a little bit of magic, a lot of trying different things in order to to be able to do that effectively. Awesome answers. I love I love this attention to detail and care for building these for building these agent systems. And I I think that there are there used to not be very many frameworks or perspectives on it and now kind of the the people who have been doing it actively in production are are able to get out those insights into the world. So it's been great to see how these best practices have been evolving. next question for for Julian for Allan. What fundamental limitations and risks might persist for highly autonomous AI agents capable of creating their own tools? and how can these be safely managed? So instead of humans defining the tools, the agents are able to to kind of create and design their own tools. So I think we're still in the very early stages of this to a certain degree. and I think you'll see a lot of these operate with human in the loop moments. you can kind of think about this in the in terms of like a capability like deep research. you send it off to go and do something for you for a period of time and then it comes back to you with a response and with a with a structure and I think you'll we'll start to see more of these type of systems and we've actually very cleanly defined that in in the a AI agents page on Google Cloud. There's actually two different agents that we kind of use to talk about things. the classical kind of chat agent that we're kind of used to that we've been interacting with and then what we've also defined as a as a background agent and that background agent tends to kind of operate in in the background behind system often potentially in kind of like a retail setting or in a supply chain setting or or something along those lines and I think we'll see a lot more of those in the future and I think that's where this specific where a lot of these specific use cases become very interesting because they can go off and kind of do thing, but there's always this kind of human that is a specialized individual that comes to check that back before kind of any any task is actually executed. And I think that's kind of the the safety structure that is is something that's in place right now and that I I I continue to suggest to customers that are thinking about these kind of use cases to keep in mind as they as they go about them. Excellent. Ellen, is there anything you'd like to add? Yeah. it's already happening. Agents are writing code, writing tools, executing those tools. there is an opportunity to have control over the code and tools that they write and write it down and then have a human in the loop audit that. And when the agent is submitting a PR as opposed to just running things that are ephemeral, you end up with a lot more visibility and control. I think that that's going to be a pattern that we're going to see more of soon. Yeah. And I love turning on the code execution flag for the Gemini APIs where you give Gemini the ability to write, run, and then even recursively fix code if it encounters errors. oftentimes like I I might have a Python task, but I don't necessarily know what library to import. and having it be able to to kind of go through that guesswork for me, is is pretty is pretty awesome. Excellent. thank you so much for the for the community questions, Simone. next question. what are the key challenges in deploying real-time AI agents in production environments? including things like response time, API costs, and accuracy. How do AI agents handle conflicting instructions or ambiguous queries from users? And are there techniques to improve reasoning in such cases? So, I thank you for the great question, Assad. I I do think that we've touched on this a number of times throughout the discussion today and and it feels like there's always some trade-off between balancing balancing response time, the costs for the models and the accuracy. at Google, we've been investing significantly in trying to have really really powerful models while also keeping the GPU footprint low. So, I'm not sure if folks saw the recent Gemma 3 release, but we're able to have pretty impressive performance on benchmarks while still just only requiring a single H100 as opposed to 32, which is some what some of our similar competitors might require. so, I I think there there are often these trade-offs that you have to think about pretty specifically. I would love to open this up though to all of the folks who are working on real-time agents and models in production. Would anybody like to add something to help answer Assad's question? Yeah, I'll I'll jump in real quick on this. actually, so one of the things I think it's important for everyone to understand about like real-time agents is that we've we've moved into a new fundamental architecture from what we had before. So, one of the things, you know, if you think about like before you saw things like project Astra and realtime birectional streaming, what we used to work in was what we call a turnbased paradigm. There was a request, you waited, and then there was a response. And even if that request included a tool call and another tool call, there was a response. But this was what we called a blocking loop, right? That that kind of happened the way it happened. When we move into this new kind of, you know, architecture paradigm of real-time birectional streaming, we're moving away from that turn-based paradigm and we're we're moving into like a full birectional event stream. That means I am always talking and you are always listening and someone else is always talking and someone else is always listening and there are events that are just coming from everywhere at all times. And those events are timestamped and those events are IDed to be able to match up to each other. But this is what allows us to build these like really magical experiences. So if you've tried, you know, streaming with Gemini, sharing your screen and talking and doing all these things that is a quite chaotic event stream that's happening behind the scenes of lots of things coming in and out, but there's no concept of a turn really anymore. Maybe you could draw an imaginary box around a set of events and say this was a turn and that was a turn. But it's it's a really different way of thinking when it comes to like designing these agents, designing the evaluations for these agents and really understanding fundamentally how they're going to work. You know, what happens if I ask a question while a tool call is happening before the response comes back from that tool call? Do I get a hallucinated response because the grounding hasn't come back? you know, those types of things need to be in consideration now because we're in this new sort of architecture of real-time streaming. I love it. It's like a playground for distributed systems humans. Like I I I love I love these complex these complex interactions. Jacqueline, did you want to add something? Yeah, I was going to add something to the second part of what you had asked, which is how do we handle ambiguous requests from users? And I think there's no like one easy answer. I think that this is once again like defining what a good experience is in your product. In some cases, you might want the agent to just assume things so you can get the task done. And in some cases, maybe you can like recall back to the memory and like see if the missing piece was mentioned previously and pull that in. On other times, you might want to actually ask the user, what did you mean by this? Like, hey, you wanted a dinner reservation for 2 at p. m. did you have a specific location in mind? Or we could just default to knowing where the user was and suggest a bunch of options that are close by to where they are. So I think this is where a lot of that product thinking will come in also to understand when do you assume when do you want to prompt the user again when do you want to come with a bunch of suggestions and let them choose from one of those options. so lots of yeah lots of options on the table but I think you kind of need to understand what's the best path for your particular product that you're building too. Yep. And having that empathy for users and and really understanding their goals and their motivations that is that is definitely something that is appreciated in this brave new AI world. Antonio did you want to add something? Well very very quickly again I'm thinking of what Stephen was saying to earlier. the way in which we humans work is we are we don't have we don't follow strict rules or terms right this is more natural more more closer to to how we interact. even in this discussion there are multiple flows of of discussion that are not prepared in advance. So there is not a single cycle of you you ask then someone is answering and so on and so forth. But it's more natural. It's more close to to humans. More friendly. Absolutely. We're doing it live. We're doing it live all the time. Excellent. Next question. this is from Shavelin. Alan, what are the security and privacy considerations that you think about when integrating APIs like Gemini into applications that might handle sensitive user data? So especially production systems. Yeah. this is a wonderful thing to think about when you're building anything. this is not just about agents or just about generative AI. If you're going to take inputs, you need to know where you're going to write down those inputs. if you're going to process them in subsystems and hand off to other APIs and dependencies downstream. You're in charge of all of the edges of the graph of code execution that you are building. And if you own it, then you're really in charge. And if you don't own it, then you're taking dependencies on somebody else and you need to read those terms of services and those privacy documents to know what's going to be written down. So like in my case, Google Cloud, we we're very clear about we're not going to write down and train on any of your prompts and stuff. that's a big component of this is just owning those edges. I always say agent ops is is really you don't forget about DevOps, right? Like all of the traditional software best practices still apply and those tools that your agents are using are just as much in scope as the prompt that you're thinking about because user information can downstream itself into those prompts. that's a vague handwavy answer. let me give you a quick tip. if you feel like if your use case allows you to, you can transform the user input into a rubric or an ontology or whatever on the fly early on in a pipeline and then you can be a little bit more confident that the private user information is isolated to that early stage and downstream from that is just your structured data that is a interpretation. That's not a perfect quick tip but that might help. Awesome. Thank you so much for for giving that guidance and thank you to all of our wonderful speakers today. Please everyone give a virtual round of applause. I have learned so much through this discussion. and can't wait to try out some of the things that that y'all recommended. so thank you so much to everybody for for coming for coming here today for for teaching all of the you know quarter of a million of developers that we have around the world all about notebookm and project mariner and all of the great work that's coming out of Google cloud. really loved having you here today. and with that I'm going to turn it over to Anat to give us a little bit of insight about the code labs. Thank you Paige. So in interest of time since we had a very interesting discussion I will share my screen now I'll be quick with our code labs so the first code lab we talked a lot about function calling and the first code lab looks into basically building a chat interface where you can interact with your local database with natural language questions. Of course, it's a PI database. So, these apply more security considerations when you use it in production. But yeah, so going ahead into the first section of the code lab. This is where we kind of create a local database, a SQLite in-memory database to play around with. We populate it with data. So we populated with the products, the staff which can sell the products and orders placed by customers. And we insert some so we create the schemas and we add some data synthetic data into the table. And and Anant are are you sharing your screen? just just checking. Yes, I am sharing my screen. Beautiful. We can see the code now. Excellent. Thank you. Okay. so yes. so let me just start over. so the first part create talks about creating a local database. We populated with some synthetic data for three three different tables for products customers who bought the products as well as stuff which sells the products. then we define basically now that we have created the data database and populated it. We defined the database functions which the model especially Gemini 2. 0's and O's native function calling capabilities would be using to interact with the database. So we define functions around listing the the tables to see what tables are present describing the tables and the schema as well as of the various fields the data types of the various fields executing queries for example select all etc. And now that we've defined the function cause through which the model can interact with the database, we go into inter implementing this together with Gemini. And this this diagram here defines kind of the function flow of how this workflow works where user kind of we define the user kind of provides a natural language input to Gemini. it calls unnecessary functions get the response back and uses the response to provide a natural language answer to the user's question. So here we initialize the Gemini 2. 0 you know API with the various tools and the instructions for the role of the chat chatbot and we see here that when the user asks for a question like what is the cheapest product Gemini executes the logical figures out the logical sequence of function calls which you should do provides it the necessary data gets the response back provides you the answer similarly for other queries as well and what is very very interesting Something you can do is you can inspect the conversation which means you can see the reasoning steps that were used from how it mapped from the natural language query to the function calls and the the output and how the model reasoned that it needs another function call etc all the way till it arrived at the answer of your questions. Now another point in this collab which is new with Gemini 2. 0 is that you you can use the live stream sorry live API Gemini 2. 0 live API to actually do compositional function calling which basically means that it not only does the function calls it also generates the the code for that function call and to arrive and after that function call to arrive at the answer which the user is looking for. One example which I really like in this collab is give me a second. Yeah towards the end yes there is an example where you ask to figure out figure out the number of orders that are made by each of the staff and then generate and run some code to plot this as a Python seabbond chart. So all of this not only interacts not only interacts with the tools and the functions provided to it but also writes code to like plot the necessary plots as as instructed and this can be done in real time. right now in the example is with text but you can do this with voice audio input as well. moving on to the second code lab. in interest of time this code lab is quite big and has a lot on lang graph. So, I'll be very quick, but this talks about building a barista bot where you can kind of have a barista bot order like order coffee and other drinks for you as well as play provide the various functionalities the user can interact with in order to fulfill the order. So the the first part of the collab basically talks about defining the core instructions which you would like the bot to follow and provided the various tool instructions. For example, you can use the place order to place order get order, add to order, confirm order, all of the things which a barista chatbot would require. Then you go all the way from single turn chat bots all the way where you have just a chatbot answering you to multi-turn ones where you have a back and forth conversation and then all the way till the end where you kind of provided the menu which the barista bot can can dynamically retrieve from a function call. and and in the very end we see that all of this is all wired together with the nodes and the edges in the langraph structure which to provide a system which which is dynamic where the you the human can interact with the chatbot. the chatbot based on the request and the context so far what the the actions human has taken. place uses various tools and and retrieves the necessary information and interacts with gives the response back to the human. Human can gives further instructions all the way until a terminal condition where human says good is reached and where the process ends and in a real-time system maybe your coffee is ordered. So thank you. That'll be all and off to you Paige for the pop quiz. Thank you. Amazing. Thank you so much for the walkthrough of the code labs and thank you to Mark McDonald for for creating them behind the scenes. Mark is one of our colleagues based in Australia. So if folks are are based around Australia, you have you have teams around the world that are that are working hard to bring this generative AI course to you. so I'm going to to start now with the pop quiz. and pull up my screen. so for our pop quizz's first question, this feels very existential. What is a generative AI agent? Is it A, a type of machine learning model, B, an application that can observe the world and act upon it using tools, C, an LLM that can only be used for text generation, or D, a type of hardware for training LLMs. Hopefully folks were paying attention today in the Q&A section as well as in the white papers and code labs. and I'm going to count down five, four, three, two, one. Make sure to note your answer. a generative AI agent is B. An application that can observe the world and act upon it using tools. Question number two, what is the primary function of the orchestration layer in a generative AI agent? is it A to generate creative text and images, B to manage the agents internal reasoning and planning process, c to interact with users through a conversational interface, or d to store and retrieve data from external sources. so everybody jot down your answer using either you know pen and paper or just kind of thinking about what your answer might be. I'm going to count down. 5 4 3 2 1. And the primary function of the orchestration layer is to manage the agents internal and reasoning and planning process. Question number three, in what scenario might the developer choose to use functions over extensions and a generative AI agent? is it when the agent needs to make multiple calls in an API and a sequence? Is it when the API requires real-time interaction with the agent? is it C when the developer needs more control over the data flow and API execution or is it D when the API is easily accessible by the agents infrastructure? Hopefully folks were paying attention to Julia and Patrick and Allen sections. when might a developer choose to use functions over extensions? and the correct answer is C. When the developer needs more control over the data flow and API execution. and hopefully folks are playing around with function calling with things like MCP servers. I'm really excited to see what you all build. Question number four, what is the purpose of data stores in a generative AI agent? Is it A to store the agents configuration settings and parameters? Is it B to provide the agent with access to dynamic and up-to-date information? Is it C to execute complex calculations and data transformations? Or is it D to monitor and evaluate the agents performance over time? counting down five 4 3 2 1 it is B to provide the agent with access to dynamic and upto-date information. Question number five, which of the following is not a characteristic typically associated with generative AI agents? Is it A that they can create new content such as text, images, or music? B that they can learn from large data sets to identify patterns and generate similar content. C that they possess inherent consciousness and understanding of the content that they create or is a D that they can be used for tasks like translation, summarization, and content creation. so remember this is not a characteristic typically associated with generative AI agents. I'm going to count down five 4 3 2 1. sometimes I feel like we should have Jeopardy music, but C, agents do not possess inherent consciousness and understanding of the content that they create. six, what is the mixture of experts, or mixture of agent experts approach in agent development? Is it A, combining multiple language models into a single agent? B using a variety of tools to enhance agent capabilities, C combining specialized agents each excelling in a particular domain or task or D integrating different reasoning frameworks within the orchestration layer. think very very hard. remember back to the white papers and all of the the conversations that we've had today. 5 4 3 2 1 it is C. combining specialized agents each s excelling in a particular domain or task. and with that, thank you so much for joining us here today. We are overjoyed to have all of you kind of here and learning all about generative AI and how to use it in your own projects. And we can't wait to see you tomorrow where we'll be covering even more topics including white papers, code labs, and more. So, thank you so much and see you later.