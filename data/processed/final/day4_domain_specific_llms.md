Title: DAY 4 Livestream - 5-Day Gen AI Intensive Course | Kaggle
Channel: Kaggle
URL: https://www.youtube.com/watch?v=AN2tpHi26OE
Caption Type: auto-generated

======================================================================

## Cleaned Transcript

 Hello. Everyone, welcome to our fourth day of the Kaggle Generative AI intensive course where we have been teaching over a quarter of a million of you developers out into the world all the things that you would ever want to know about generative AI and how to use it as part of your projects. I am so excited to be here today with our expert panel of guests and all of the folks who are around Google Google DeepMind, Google Cloud, Kaggle, all sorts of places in and alphabet who have come together to build this course and to make it a reality. So just as a general reminder, this course is a 5-day generative AI intensive sponsored by Kaggle. it includes daily assignments, things like discord discussions as well as code labs. we are on our fourth day of the course which is focused on domain specific large language models. so showing how you can fine-tune models to to meet the needs of your projects. And I want to give a huge virtual round of applause and thank you to our wonderful Genai course moderators. They have been working tirelessly around the clock to answer your questions to make things to make things perfect for our code labs. and I just want to to make sure that they all get recognized. So, thank you so much to everyone, especially our ops team Brenda Flynn and Kel Perk who have been tirelessly again just pushing through to make this happen and giving a lot of love to the course. so with that, I am going to hand it over to Anant who is going to give a brief curriculum overview of all of the white papers that y'all have been reading over the last few days. Thank you, Paige. So, hi everyone. So, welcome to day four. So in my curriculum overview I'll just start off by mentioning what we have done last few days. For instance in day one we looked at how foundational models take and various topics such as techniques to tune them make them run efficiently as well as prompt engineering to get the desired outcome. Then in day two we looked at how data can be embedded in their semantic representations and search fall at scale using vector databases. And then in day three, yesterday we looked at how all of this can be combined and extended to make agents which can dynamically make decisions and interact with the external worlds in order to achieve their objective. Now today in day four we are going to look at how all of these concepts that we learned in the previous days can come together and being used to make specialized domain specific models. So let's get to the recap of the white paper for today. So first we read in the white paper that general LLMs often need specialization for fields like cyber security and medicine as well as more. These areas have unique data challenges, technical language as well as sensitive requirements that demand this adaptation or can benefit from it. Then we saw we started off with the cyber security example with SELM. It tackles evolving threats, analysts toil and talent shortages by combining specialized models, threat intelligence as well as automated reasoning to assist security professionals. Training involves adding security data and fine-tuning on relevant tasks and some more. Then we looked at the section on medicine which features Medelm and Medp. These models address the challenges of navigating vast medical knowledge for tasks like highquality medical Q&A and supporting clinical workflows. Medp 2 for instance showed expert level performance on medical exam benchmarks. A key to this field is rigor rigorous evaluation and we read about this for instance using specific benchmarks and expert clinician reviews plus careful phased real world testing as well. Now training involves instruction fine-tuning on medical data and advanced prompting strategies for these two models. The white papers then basically towards the end concluded that with the fact that successfully deploying these powerful tools requires not just specialized models but also deep collaboration with domain experts and thorough step-by-step evaluation to ensure safety and effectiveness in these critical fields. So that would be the summary of what you read in today's white paper. And off to you Paige for the Q&A. Amazing. And that is super exciting. I loved learning all about how we can fine-tune models and make them great for things like cyber security and things like healthcare. and with that, we're going to go in our Q&A session. So, I am overjoyed today to welcome Scott, Chris, Caric, Eva, and Donnie to talk about the work that they're doing around Google, around Google research, Google DeepMind, and Google Cloud and to tell us a little bit more about these fascinating topics and all about fine-tuning and how to make it real in production environments. So, welcome so much today. for our first question, Donnie, this is for you. from a research and engineering perspective, what are the most effective strategies for specializing LLMs? Is it better to fine-tune large general purpose models to train smaller models from scratch on domain data or use techniques like parameter efficient fine-tuning? Do these choices kind of impact performance cost? and how do you keep the models updated over time? so lots of lots of questions embedded into a single one. Do you want to take that? Thanks so much, Paige. So, what we found over time, especially in our experience with building Medp and MedLM, is that there isn't one best way to specialize a large language model for a task and that the best method really depends on the nature of the task, the complexity of the task, and of course, like the amount and quality and the nature of the data that you have available to you. the best way to figure out which method works best is well first of all you can look at similar problems other things people have tried things in the literature and see if any of those are a good fit and then try that first but ultimately you really just have to experiment and see what works best. You have to test different methods and and see see what works well. Of course you know the less compute that you need to achieve your goals the better right that allows you to control your costs. that also allows you to keep your you know g give you gives you the ability to keep your model better updated if if it's cheap to to retrain but some complex use cases will require regular updates of new data and so you'll be you know you'll have to be training regularly to to keep that model updated so that affects the updatability as well thanks for the question awesome and I know that I I often have seen people ask unfinet so just the the Gemini models or others like it medical questions or things that are are similar to healthcare and grounding on PDF documents or other kinds of data. would you recommend that people always start with kind of prompting the the unfineted model before diving into fine-tuning their own? It's always good to see what the the base capability of the foundation models are. I think in specialized domains I think you'll find quickly that there are going to be limitations especially if you're thinking about you know about deployments especially in clinical scenarios that you're going to test the foundation models they're going to be very good at many many many many tasks but there's going to be specialization that you're you're you're going to find once you kind of hit the edges of what the the foundation models are capable of. There are of course use cases where the foundation models are going to are going to do very well and and that will you know that can be the end of it and you can use those in your applications. Yep. I love being able to to kind of turn all of these knobs and dials to to get a model that's really really compelling for a business use case. and that we have this flexible series of approaches across just using models out of the box and then also customizing them for user needs. So that's that's very very cool. Thank you so much for your answer. our next question is a community question from Yuvan Theron. during the fine-tuning code lab I observed significant token savings. However, there are scenarios where fine-tuning can are there scenarios where fine-tuning can unintentionally reduce the generality or creativity of a model and how do teams evaluate when to fine-tune versus when to use prompts or retrieval. So very very related to the last question that we were discussing. Chris, do you want to talk about this one? Yeah, sure. I think this is a this is a great question and there's definitely problems that you're going to encounter when you when you start fine-tuning. especially, you know, there's a there's a problem that you'll probably hear commonly referred to as catastrophic forgetting, which is sort of kind of the the worst case scenario you you encounter when you're fine-tuning where you've gone too far and the model can't do other things anymore. It's only able to do kind of the very specific task that you fine-tuned on. and there's of course, you know, kind of a gradient in between the foundation model and the, you know, the kind of worst case scenario I just described. but especially when it's when you're fine-tuning, it's really important to have a thorough evaluation around a range of tasks that you're going to be doing. Not just the task you're necessarily fine-tuning, but what what other possible scenarios you're going to be using it in, so you can understand kind of the the quality either gains or losses depending on how you've been fine-tuning on those sort of adjacent tasks. generally when we're doing this in in security use cases we aim for a range of evaluation tasks and also a range of fine-tuning tasks. So we want to teach it general concepts during fine-tuning and then be able to sort of use the generality of those those those concepts again at you know at deployment time or at runtime. It gives us kind of a mix of both. but really evaluation and I think you're going to hear this in a few different areas this morning but evaluation is key in a lot of these kind of domain specific areas. Awesome. And Scott, did you want to add something? Yeah, absolutely. So, just to build on sort of Donnie and and Chris's answer here. I I always go back to I know this maybe this is like not not the cool thing to do, but I always go back to like the the first principles of like ML, right? So, does the model, you know, is the task that I'm doing in distribution for the model? Was it was it trained on the thing and on the data that I expected to operate on? I think that's really the the key question when it comes to fine-tuning and and which level of fine-tuning, whether it's full parameter fine-tuning or something is parameter efficient like Laura or other methods. so I think one thing you'll you'll find there's a really great paper out there called the reversal curse. that kind of starts talking a little bit about where in this stack of training you can start to actually get really good generalizable knowledge. And sort of the further down you go from you know pre-training through fine-tuning through you know alignment and now into you know parameter efficient techniques like the less and less generalizability you get and the more like forgetting you tend to get and the more honestly the more indistribution your task has to be in order for it to be successful. So, I think that's one of the big questions to really think about when you're when you're talking about kind of like to what degree do I need to fine-tune is really like, well, do I already expect this to be in the training data somehow? And if so, then maybe I don't need to do that much to just nudge the model a little bit in the direction that I want it to go. And if not, then probably something more significant is going to be necessary and we'll have to do that more carefully to not forget some of the baseline capabilities. Amazing. and we'll put a link to that paper that you that you recommended into the into the YouTube video and some of the notes for the course as well. I love that answer. Excellent. Thank you all. This is a great start for our Q&A. next question also for Chris. so when developing LLMs for security domains like SECLM, what are the biggest challenges in acquiring and using relevant highquality data for fine-tuning given its sensitive or proprietary nature? How do you measure success and ensure these models improve security operations without introducing new risks? So, it's a very eval discussion this morning. Yeah, def definitely I think that's great though. so we we know that providing accurate and useful security advice is really hard. you know, there's a lot of convicting opinions online. Everybody has sort of different ways of approaching security and it's also something that really needs to take the kind of environment and situation into account. you know thinking about people at home you know and email security is very different than thinking about code security and other sort of tasks. And so we we do need to kind of sort of have different kinds of data sets for different kinds of tasks. And oftentimes those data sets are are proprietary or or or sensitive in in some way. So what we've sort of done with building SELM is have a chance to experiment with that whole spectrum. So everything from training and fine-tuning on models where we are using heavily you know a large large amounts of internal information about security and threat actors and sensitive you know sensitive material there all the way to the kind of the other end of the spectrum using basically foundation models more or less off the shelf. and that gives us a really sort of nice spectrum to sort of pick pick a and place to work. and so what we've seen is that the best performing approach for us is kind of sort of in the middle where we're doing training with plenty of high quality public security information. and that's really feeding into you know into the foundation models into Gemini itself in this case. and then we layer on runtime techniques like rag. That doesn't mean we don't need fine-tuning. We do think about fine-tuning absolutely as how to make these tasks perform well. But we tend to fine-tune in a way where we're looking at sort of understanding a generalized task rather than maybe memorizing specific information. So it's it's not about necessarily the recall given you know the recall against a specific threat actor or other s sort of p piece of material but being able to converse and answer questions accurately giving the grounding material for about that particular actor in that in that in that situation. the other the kind of other element of sort of coupling these training fine-tuning and runtime runtime techniques gives us a really nice way to also give our our users other sorts of benefits. So features like citations provide a lot of user confidence in sort of what the model is telling them. you know, if we if we give people advice in a in a security direction, you know, you should change your password so and so often, it's really great to be able to point to a policy, a document, research, other things that that kind of vouch for that particular that that particular statement. and so that that's one way that we do this. And of course, evaluation on this again is key. So evaluating with with that grounding material, without the grounding material, all of that sort of thing, you're going to build out really good evaluation data sets for understanding really just how how good you are at the different security specific tasks with and without that proprietary information. Yeah. And that's a great answer. And for folks who are curious about evals, the next day is going to be discussing them in depth. So make sure to tune in tomorrow as well for for more discussion about evals. I also really love citations. I use them all the time. both when grounding with Google search. So, being able to see kind of the up-to-date and relevant information that the the model is using to help ground its outputs. as well as grounding on on internal data sources. I love being able to to kind of click and get to a PDF document or something similar to to get more confidence in the model's answer. awesome. Next question. This is for Eva. healthcare and life sciences is a domain with complex terminology, high stakes for accuracy, and significant regulatory considerations. What unique challenges and opportunities arise when building or fine-tuning large language models for these healthcare applications? And how do you ensure that the model is reliable, that it has high high safety and also compliance? thank you, Paige. So, we've been hearing a lot about the challenges. So why even do domain specific LM? So let me start with the potential. I think that the potential is immense. Think about personalized medicine. Think about scientific discovery, improved clinical documentation, more effective health care processes. There is so much potential that building those LLMs brings and hopefully many of you are looking at how we can bring this technology into the space of healthcare and life sciences. the challenges unfortunately are still pretty hard and unique complex domain specific terminology. if you think h about how two doctors talk to each other, none of us could understand that and the LLMs need to be able to actually understand that new terminology. High accuracy demands this is actually a life and death situation in many cases evolving knowledge landscape. the science both the medical science and the life sciences evolve and new discoveries come up every day. So the stale information becomes inaccurate. There is data scarcity need for causal reasoning and really stringent regulations. so the opportunities lie in automating tasks, accelerating research and improving patient interactions. it's really important to ensure reliability, safety and compliance. And in order to do that, we use a multiaceted approach. This includes curated data with strong governance rigorous model evaluation with specific metrics and expert validations. we're prioritiz prioritizing explanability, implementing robust safety measures against harmful and harmful content and biases and adhering to all regulatory requirements. there is a necessity for interdisciplinary collaboration and continuous monitoring. So not just a training time and fine-tuning time, but you want to continue moni monitoring the systems after deployment. Awesome. That's a lot of wonderful things to consider when when building these healthcare and life sciences AI systems. So thank you for going into depth for all of the things that engineers might want to be mindful of. next question is for Caric. welcome welcome to the call. how do you balance the need for deep domain expertise within the model against the risk of things like catastrophic forgetting which we heard about about a little bit earlier or overfitting on specific training data? are techniques like rag or using the context window sometimes a better alternative or a complement to fine-tuning for domain adaptation? Thank you for the question. That's a really great question. I think let me answer that in two parts. let's start with the first part. Right. some of this was discussed by DA and Scott and Chris and like the first set of answers. Right? I think there are some very important practical considerations when you make some of these choices and it depends on how you plan to use the final model. What is the domain and specific applications of this model? If your goal is a very specific, very narrow task that you don't run the risk of sort of needing a lot of general purpose knowledge, it is okay to sort of go with a more heavy-handed fine-tuning approach per se. because you you there usually isn't too much in terms of a trade-off outside of computation budget. but a lot of tasks that do need more open-ended, more knowledge heavy tasks do sort of need a more gentle measured touch because you do really want to sort of make sure the model is not losing some of the general purpose knowledge because again as already we discussed the more you fine-tune the risk you run of sort of losing some of that knowledge. So I think there also is a consideration in terms of the out ofbox skills for the base LLM on the specific domain or application a domain that is more commonly represented say in perhaps in the pre-training data is going to be something which is better models are going to be better at say something like code than they would at if I ask you something a very esoteric domain per say I don't know nuclear physics or something like that. So, I think that is another consideration when you're sort of trying to determine this how common popular how good are the out of box skills of the base model before you sort of try to sort of go further down the fine tuning route. in the recent years I think the LMS have advanced enough that the pendulum has swung a lot more towards light touches parameter efficient finetuning. just because again the base models have become much more powerful learners and the need for sort of more heavy-handed tuning has really sort of gone down as the base models seem to be able to learn much faster with far less data or in some cases just out of the box. the second part, okay, this is really great. I love I love this question. because I think techniques like rag are somewhat underused honestly. newer models have become much better at using the information provided in the context. I I can I I'm reminded of an example a demonstration that was provided about a year ago per se, right? where someone took a grammar manual for this language called Kalamang which has only fewer than 200 speakers worldwide. They added that to the model in a single prompt and the model learned to translate from English to Calamank at a similar level as speakers of that language which is crazy to think about it right there was no fine tuning nothing you just gave it the manual I think this is an increasingly important capability of models techniques like in context learning really allow the model to solve new complex tasks with just a few demonstrations and again This is a very actively growing area of new models as they come about and like if anything I expect performance will keep improving in these areas. Rag is another technique in the same vein right where you can put in a lot of valuable information into the context and hope that the model can learn from those valuable pieces of like the snippets of information that you have provided there. At the end of the day though there's also a practical question here. are you willing to pay the inference cost because typically when you have techniques like rag and context learning you are working with slightly longer prompts. it again here too I feel the pendulum is swinging more towards allowing these techniques just because techniques like prefix caching and other optimizations and inference mean that LMS have become a lot better at dealing with such longer context. the older days of transformers like we no longer have the quadratic attention setting where like you can you can't scale up to super long sequences. I mean the Gemini models can handle up to a million tokens easily. So I think it then gates on your ability to find useful information and again this tends to depend a lot on the actual specific problem and domain. So yeah. Yep. I I love that and I'm mute I am a huge fan of Gemini's long context window especially for things like pulling in entire code bases similar to the example that you gave. I've really loved being able to pull in Cobalt code bases along with just kind of an instruction or a documentation manual. and then suddenly Gemini knows Cobalt or it knows Cobalt much better than it did before. so if if folks are a little bit time crunched and they don't necessarily have the ability to or the TPU resources or GPU resources to do fine-tuning, I think sometimes testing out things in the context window is a is a really great first kind of prototyping phase as opposed to as opposed to to starting immediately with fine-tuning. Awesome. next question is our second community question from Unatara. How is data grounding handled when external knowledge bases are real-time data streams? What is the accuracy level for for those systems? And Scott, do you want to take a take a stab at this one? Yeah, absolutely. so I don't think I have any good metrics that I can share in terms of accuracy levels. I do I do I can say that for SECLM that sort of grounding information is incredibly important for all the reasons Chris mentioned at the very beginning. just the the amount of change that happens in this domain from even hour to hour in terms of you know frontline observations of attacks and threats is really important to be able to bring that information in in a sort of a real-time fashion. I you know I kind of I don't know maybe others on the call will will kind of feel differently here but like the real time aspect actually doesn't change much in my mind in terms of how you deal with the data right ultimately you're still going to want to perform retrieval right which means taking whatever is in the prompt from the user and kind of matching it up against this data somehow right so you can imagine caching mechanisms or other data stores that you would sort of update over time as you get the you know new new items in the stream and and so on and so forth. I think actually if I might co-opt this question slightly though I think one interesting thing to think about here is like specialized versus less or sort of generalized grounding kind of ideas. you know, I think there's, you know, an interesting kind of distinction here where, you know, you have Google search grounding which allows you to obviously get a large amount of relatively generalized information about a topic and then you have, you know, a little bit more cost that comes in with like a specialized knowledge base, right? So you know in SECM for instance we use you know we use our internal mandient threat intelligence knowledge graph as part of our augmentation. and you know one of the interesting things here is like sometimes the way in which the cyber security cyber security community refers to something is like fairly general. So, there's a lot of naming for thread actors of things like Cozy Bear or like, I don't know, like winter sandstorm. There's a bunch of these kind of a little bit odd actor notions. And if you just go and search these things up on, you know, Google search grounding, you're going to get a whole bunch of really unrelated pieces of grounding information. Right now, the model usually can sift through these things and kind of identify when it's useful and when it's not useful, but sometimes it can and sometimes you get some pretty wacky stuff. So I think one interesting thing to think about here is is you know your kind of domain separation in some ways where you do have your very well-defined authoritative sources where you kind of curate it and make sure it's very high quality and sort of use that as your primary source of information and then you have something like Google search grounding which you can use to kind of shore up any gaps that you might have in the the underlying knowledge store. I will say one thing that the real time kind of probably introduces here that maybe I missed a little bit the first part which is the sort of temporal nature right which is that you know you could see many examples of the same entity pass through of this this you know real time stream with like updated information over time. you know I think that there's an interesting sort of engineering question there about how to integrate that best. But again to me this feels a lot like a you know how do you keep your database up to date or how do you keep n you know replica replicas of the last you know iterations of some entities updates in this real-time stream in your in your database and retrieve them effectively or at least you know show it to the model in a way that it kind of captures that temporal change for that model over or for that entity over time. I love this discussion about retrieval and kind of augmenting the models outputs with with up-to-date information. And hopefully folks remember back to day two where they learned all about vector databases and embeddings and grounding models in in data sources. So this is this is great discussion. next question. Google Cloud Security uses Seculim technology which integrates Google secops, Google AI infrastructure and mand thread intelligence leveraging TPUs. How do you manage the space complexity? Scott, do you want to take this one? Yeah, very carefully. yeah, the the you know again you know I always try to boil things down to like what's what's existed in the ML space for many years, right? So I think tomorrow is the MLOps session and I think there'll be a lot more discussion about some of these concepts in that in that section. But like in my mind, a lot of this complexity eventually boils down to a really really really challenging case of MLOps, right? So in a traditional ML system, you have to monitor your data sources to see if there's changes, right? You have to keep your model up to date and do various types of evaluations in both real time and offline to ensure the quality of your model doesn't degrade over time. These same things exist, right? Maybe this time you don't get to choose when the model up updates or choose the level of degradation that you that you can absorb, right? But at least you can monitor all these things. So at the end of the day, it really comes down to like you know having these processes but at a pro at a level that's like much more expansive, right? So in the past if you had like let's say an image recognition system, you'd have a stream of new images coming in that you would evaluate against. Maybe you you know cut off some some subset of them to train. Well, here now you're getting, you know, you have all of your input feeds, right? So, anything that you're using from a tool or from a rag store that you have to monitor, which could be many, many sources for for SELM, for instance, we have, I don't know, at least half a dozen or more different API feeds that we that we monitor. you have your other sort of static rag stores. You have Google search grounding. You have the models themselves which actually, you know, from version to version can demonstrate some pretty substantial behavioral changes. And you basically have to be able to evaluate in a in an auto automated fashion like all of these things and still be able to actually pinpoint, you know, hey, this thing is degraded on code use cases. This thing is degraded on our threat intel use cases. and then be able to sort of further unravel the sort of knot and say, okay, well, if it's a thread intel, you know, regression, like, okay, was that because the data changed? Was that because the model is not operating the same way on that same data that did in the past? is there new data that that didn't previously exist? for you know another you know we talked about the temporal aspect in the last question that also plays here right which is yesterday an IP address or a a host name or you know domain might actually have been considered malicious because at the time it was actually doing malicious things and in that time between yesterday and today's eval that might have changed completely right that that might have been mitigated or someone may have put some other remediation on it and now it's no longer malicious and all these things kind have to be wrapped up in some holistic eval system so you can kind of really understand what role each component of your system plays and and how you can address it to kind of keep high quality. Amazing. As a distributed systems nerd, this makes me very excited but also very apprehensive. So this is this is great this is great to hear. and it also it also I think is a lot more sophisticated of an approach than what we saw, you know, even just a couple of years ago when people were first starting to test out these large language models. it's really wonderful to see how models now are just becoming part of a system that that's a bit more representative of real world use cases as opposed to to just being the end all and be all. so this is this is wonderful. Thank you for the great question, Dina Carr. next question from CX Wong. What are the design considerations on when to apply a domain specific large language model versus generic LLM in hybrid situations and use cases or are there hybrid architectures available that combine both? And we've already touched on this a little bit. Caric and Eva, do you want to do you want to take a stab? Sure. I guess I can jump in. so yesterday you heard a lot about agent architecture. So obviously agent architectures are a great way to combine general and more specialized models into a hybrid solution. If you think where the strengths of a general model are, they're in the con conversationalist kind of the user interface to a deeper system. And then if you have an orchestrator, you can easily have multiple domain specific LLMs behind that orchestrator. And you can keep those LLMs much smaller and specialized by not even necessarily having the general knowledge because with Gemini's large context window, the orchestrator can bring in the right domain information and your generalist conversationalist can actually translate that into a language or a process that's more general. So, so that's probably the best type of hybrid architecture is using an agent type of architecture and this also allows you to bring in data of and and kind of deploy rag type of approaches to augment with additional information in real time. so, Cartek, do you want to add some? Yeah, thanks. I think that's that's a really great answer. So I think the only thing I would add to that is it it would depend a little bit on sort of like things like the practical constraints like compute or sort of like the complexity of the system you're targeting. agentic architectures are really sort of where the world is moving towards per se but it is not necessarily always like the easiest things for folks to start with just quickly play around get a quick and dirty solution per se. again I mentioned rag as well that's that's a really nice way of thinking about it. I think it depends a lot on how deep domain knowledge a specific task would need which could influence the solution. If I give example earlier of sort of if you had medical professionals speaking in their jargon per se that's not really something which is very easy for us to sort of have a general purpose other than figure out okay this is what is being spent by person A when they're speaking to person B right and I think that is I think where some of these decisions need to be sort of like factoring in sort of like how much deep domain knowledge is needed but again I think the hybrid architectures like as Eva said like which agent authentic sort of like like the more agentic settings are offering is really where I see the world moving. Yeah. Amazing. Like I I think this is this is wonderful to have these kind of coupled approaches that help real people solve real problems. So thank you both for the excellent answers. the next question is from Batman. Batman. so so we have we apparently have a superhero on our discord channel. with the rapid advancements in large language models for domain specific applications, what are the key limitations preventing their widespread adoption in critical industries like healthcare and finance and how can we bridge those gaps? Donnie, do you want to talk about this in the healthcare domain? Absolutely. my toddler will be very excited to know that I answered a question from Batman today. so I'll talk mostly about healthcare because that's the domain I know the most. But I will say that in the financial services there's a there's a very strong parallel in terms of financial health and financial data leading to to assessments of financial health that that are very applicable here as well. so I'll kind of break down the the key limitations into three categories. one of them is just the the complexity of the domain which I think Ava has already talked about and the solutions that she talked about were exactly the things that we actually had to do for med right to evaluate with expert evaluations. A lot of that had to go into making medal and medelm things that we felt comfortable letting developers have access to. the second layer of complexity is really a complexive solution. If you think about a, you know, a health encounter, a clinical encounter, the the model, the model prediction is just a very small piece of how that integrates into an entire solution, an entire workflow. And, you know, we talked previously about agentic architectures. We haven't really completely solved the problem of what are the right types of architectures or what types of agentic techniques need to be brought to bear in order to solve a a a system problem or a solution workflow level problem you know within within a healthcare system. And then I think the last layer of complexity is really systemwide complexity. And here it's not just like you know even if you have a deployable solution even if you have like oh yeah we can you know for this particular use case we can do the same thing that a doctor does. There are so many stakeholders within within the health system not just regulatory which A also talked about you know like health care payers, governments, hospital systems, patient advocacy like a lot of stakeholders need to have their concerns listened to and understood within the framework of that solution and you know and and and making sure that the solution has you know you know respects all of these different stakeholders and their needs is also immensely complicated. So thinking about bias, compliance, usability, explanability of the solution, safety, you know, the ability to integrate you know across across a lot of different platforms. So all of those factors come together to to really amp up the complexity of of the solutions and the the needs here. Thanks. Amazing. And that was our last question for today. Thank you so much to everyone for for kind of discussing everything that you're building and everything that you're creating in the research and the production world. I really loved the conversations around agent and hybrid architectures in the medical domains and healthcare as well as in the security world. It sounds like a a really honestly very fun and exciting time to be working on these systems. Kristen Scott, did you have anything that you wanted to add about these these kind of agent and hybrid architectures? Yeah, I mean I I think I our our journey on the SECLM side is probably very similar to the metal side, right? I think we learned very very early on in the in the process like whatever 2022 or whenever we started kind of really going down this path is that you know one one model probably will not solve it all right we can't do the Lord of the Rings here with the model and so it we we definitely got to sort of this agentic sort of tool use style architectures very very quickly because it does allow us to kind of very pointedly inject kind of exactly the expertise we we need and exactly the right places. Right now, I think Paige and others have have said that there's definitely like a trade-off here, right? Like you're kind of you're trading off like the simplicity of just having the one model and doing the one thing and being able to capture that very nicely with Eval with something that's more bespoke and definitely going to have more technical debt associated with it, but it's going to be probably more well-designed to exactly the use cases you want. And I think that's very important when considering the sort of fine-tuning aspect of of the LLM. Absolutely. So with that, thank you all so much, for coming and for, for sharing everything with us today and for sharing all of, all of that the things that you've built with our over a quarter of a million of developers out in the world. we really appreciate you. Thank you for taking the time. and we can't wait to to kind of test out all of the great products that you've recommended. so u have a great rest of your day. and we'll get right into the code labs where folks can learn how they can start building all of these experiences and approaches into their own projects. so thank you and Anant dive into the dive into the notebooks. I'm really excited to learn more. Thank you, Paige. I'm sharing my screen now. all right. So, we had two code labs for today. covering a lot of the concepts that were talked about in the white paper and this super interesting discussion actually. So, yeah, let's dive into it. So, the first code lab I'm going to be discussing is the one about fine-tuning a custom model. and this code lab in this code lab we're going to be just using the same data set the news group data set from day two where we used an embedding model and train the caras model on top of it this time around we'll be using Gemini both the base Gemini as well as the fine-tuned Gemini for this particular classification task so the problem remains the same but we using a different approach to to solve it so moving on so in the first part of the code lab just like in day two's code lab we download the news group text data set and this basically has some news group posts which which can and and there which need to be classified into their respective news group categories or news groups and you can download this data set the train and test subsets of it and yeah so for example these are some news group news groups or news group categories which we expect the model to classify correctly ly against. So the next part of the collab we prepared a data set to both clean the news group posts which are being fed as the input to the model. However, there is also another aspect to it. the cleaning helps us or pre-processing helps us prevent the model from shortcutting or directly from the text to the to the target news group may so because sometimes there you known users of a forum can be used to guess which news group the text belongs to. So we want to test the ability of the model and its ability to understand the context. So we are removing that additional information from the text. Then after all the pre-processing it looks something like this. The data frame where text and the respective news group news groups or news group categories. And then now we are going to train parameter efficient fine-tune the Gemini 2 sorry 1. 5 model and in this case we will keep 50 rows from each category for training. we need now keep bear in mind these 50 rows are significantly less than the amount of rows we used for training the kas on top of the embeddings in kas model on top of the embeddings in day two. That's because the model the base model already has quite a lot of knowledge embedded into it and we do not need to use a lot of data to to fine-tune it with parameter efficient techniques. So so this is what we do here and then before we get started with tuning and like evaluating the tune model it's good to always have a baseline in this case our baseline could be should be as well the the the base model with just a zeros classification approach. So if you u if you just pass in the text the news group text as an input it it basically gives an answer to the to the question or gives an answer to this. Basically if you give it a text it does not give you the class. We do not want it to answer to the news group post. We want it to classify it. So here's where your prompt engineering knowledge from day one comes into play where you ask you give give it a prompt like which what news group does the following message originate from? And using Gemini 1. 1 flash we see that it starts giving some news group. It is quite verbose. We just want the news group. We do not want any extra information. So you could also try and parse out the relevant text or refine it further by giving it some role the role prompting which we learned in day one. And and and then you see that it gives slightly like more more concise responses. But this is not always correct. still. So if we evaluate the overall accuracy of across a test set of just using a zeros prompting for classification, we see it's not not very high. And now if we tune a we do parameter efficient finetuning on Gemini flash 1. 5 flash then we see it improves the results significantly. And there are a few knobs that you can tune such as epoch count which tells how many times the model looks through the data batch size and more which you can use to get better results. They they would you need to just like in predictive models you need to play around with them to see how how much they can improve the performance. Now so after after the new model has been tuned or the parameter efficient fine-tuned we see that the performance in improves significantly and that's where we see that in such scenarios parameter efficient finetuning could be useful. Now as discussed earlier in the Q&A as well as as well we see that there's another benefit other than performance to to do doing fine-tuning for models and that is the reducing the token count. So because we do not need a very very elaborate prompt at the input side as well as the model is less verbose on the output end the model so the model generates so generates and uses less tokens. So the overall token computation is significantly reduced and that can result in significant token savings as we we see here. So that would be this all for this code lab. Let us move on to the second code lab which I am sharing my screen for. Second sorry give me a second. yes so I'm screen for the second code lab. And in this code lab we look at Google search grounding and how grounding as we discussed earlier in the Q&A as well can help you improve performance. Now the first section of the code lab is an optional one of course where you can kind of try out grounding on Google search with the with the Google AI studio UI where you can ask questions and enable grounding to get more u more upto-date and more accurate responses. and you can also get citations. Here you can see where those answers came from as well as the see which part of the section the response came from which which particular source and in the second part of the collab we do this using API in a programmatic fashion and here we basically ask for example a question about a certain date when and where is Billy Elicia's next concept and we see that it does not give you any specific dates because the model is trained up to a point of time and does not have access to real-time information. Now with grounding enabled, it gives you a specific itinary of where and when the concepts will happen. Now another important aspect of the API is that it can provide you not only the response but also tell you the sources from where the grounding chunks can chunks can tell you the sources from where the Google search API provided you the response. So, and not only can it provide you the sources, but you can also go further into it and see which part of the response it can be attributed to which which particular source. And this can really help you as we see in the next part of the collab. it can really help you build this kind of citation augmented response with where you know which parts of the response can be attributed to which specific sources. and and after that in the code lab we learn about using tools together with the Google search grounding tool as well. So other tools for example the code generation tool. So in this part we see that we ask it the question for example here and then we we provided the goo Google search tool and it provides you an up-to-date accurate answer you by ground being grounded on the Google search grounding Google search results but we go a step further where we give it an access to another tool the code execution tool which can actually write and execute code for you And we say okay take the results from what you got earlier here and u plot this as a C plot this as a seaborn chart and we see here that it can generate a very nice chart for you. I personally found this super super useful. So yes that concludes our collab and off to you page for the pop quiz. Thank you. Amazing. Thank you so much Anant and thank you to Mark McDonald also who is on our team who helped build and create all of the code labs that you've been using over the last week. So thank you thank you both so much. with that we are going to go into the last part of our of our course today which is the pop quiz. so for our pop quiz today you will be tested on all things fine-tuning and domain specific customization of large language models. starting with what is the primary goal of metal in the medical field? Is it a to replace doctors with AI systems for more efficient patient care? Is it B to develop AI models capable of diagnosing and treating diseases without human intervention? Is it C to improve health outcomes by making advanced AI technology available to healthcare professionals? Or is it D to create a universal AI system that can answer any medical question with 100% accuracy? So, grab your pen and paper or just kind of jot it down somewhere or take note of it in your head. what is the primary goal of metalm in the medical field? I'm going to count down five 4 3 2 1 and it is C to improve health outcomes by making advanced AI technology available to health care professionals. so keeping a human in the loop and making sure that we get the best quality care to folks. Question number two, what is the significance of ensemble refinement in MedPal V2? Is it A that it allows the model to learn from patient data? B that it improves the model's ability to generate multiple choice questions. C that it helps the model identify medical images or D that it enhances the model's reasoning and answer refinement by conditioning it on multiple generated reason pathways. what is the significance of ensemble refinement? I'm going to count down five 4 3 2 1 and the answer is D. It enhances the model's reasoning and answer refinement by conditioning it on multiple generated reason pathways. Question number three, what is the innovative approach used by SECM to address security challenges? is it A, developing a highly complex AI model with trillions of parameters for superior performance? B, replacing human s security analysts with fully autonomous AI agents? C combining large language models, authoritative data sources, and a flexible planning network. or D creating a global database of all known cyber threats for real time threat detection. remember back to Scott and Chris's answers. and decide what is the innovative approach used by SELM. I'm going to count down five 4 3 2 1 and the answer is C. It combines large language models, authoritative data sources and a flexible planning network. And as I mentioned, it's it sounds really really cool to a distributed systems engineer. Excellent. Question number four. How does SECLM address the challenge of limited publicly available security data? Is it by A collecting and storing sensitive user data for central model training? By developing specialized LLMs trained on cyber security specific content and tasks, C by relying solely on open source data for model training. or D by generating synthetic security data to augment limited public data. I'm going to count down five 4 3 2 1 and the answer is B. By developing specialized LLMs trained on cyber security specific content and tasks. and with that, thank you so much for everyone who has joined us today. really appreciate you, your questions, all of the activity that you've been that you've been doing on Discord. and can't wait to see you tomorrow for our last and final day of the Kaggle Generative AI intensive course training. Hopefully, you've learned something. I certainly have. and we are really really grateful to have you as part of this broader learning family all over the world. So, thank you. see you tomorrow and get excited for our last and final day. Thank you everyone. Thank you. Right.