Title: DAY 5 Livestream - 5-Day Gen AI Intensive Course | Kaggle
Channel: Kaggle
URL: https://www.youtube.com/watch?v=eZ-8UQ_t4YM
Caption Type: auto-generated

======================================================================

## Cleaned Transcript

Hello everyone. Welcome back to our last and final day of the Kaggle Generative AI intensive course where over a quarter of a million developers around the world have tuned in to learn all about how you can build AI applications using the Gemini APIs things like vector databases, agents, and more. we're so excited to have kind of our last session of the week and we also have some fun surprises ready for you towards the end of the session. So, make sure to pay attention as we wrap up today and and kind of share more about next steps. as always, I'm Paige Bailey the engineering lead for our developer relations team over at Google DeepMind and I am overjoyed to welcome all of our expert guests today to talk about MLOps for generative AI. Just as a general reminder, this course is sponsored by Kaggle, which I encourage all of y'all to take a look at. They have lots of great models, data sets, and more available for you to use in your projects. this is day five MLOps for gem render of AI. So all about how you can take these apps that you've built and put them into a robust production robust production system. and I want to give a huge virtual round of applause and thank you to all of our wonderful moderators. especially Brenda Flynn and Kal who have been making everything happen behind the scenes tirelessly working to answer Discord questions, make sure that all of the logistics are wrapped up. so, if you see any of them around the digital space or in person, please make sure to say thank you. and with that, I'm going to hand it over to Anant for a brief curriculum overview of what we'll be learning today. Anant, take it away. Thank you, Paige. Hello, everyone. Welcome to the last day. Today, we'll be looking at how to operationalize Genai at scale using MLOps on Vertex AI. So in the first few days we looked at how to foundational models are built. How you can prompt engineer and craft like good prompts for for the inputs to get the desired outcome. We looked at embeddings and how they represent semantic representations of data and how you can query them at scale. Then we looked at agents of how they their dynamic you can use dynamic aspects of them where you can see how agents interact with the external world to solve the task at hand. Then we looked at domain specific models where we saw how you can make specialized models for certain tasks and fields for example medical and medicine and cyber security. And today we'll be looking at how all how we could kind of combine all and a lot of these and put them in production so you can actually start using them for your applications. So so quick overview of the white paper. We read in the white paper how MLOps principles have to be adapted to the unique genai life cycle and they cannot be used as is. For example, this includes the discovery phase of Genai, the development and experimentation phase, evaluation, deployment, monitoring and governance. These are all aspects which need to be adapted to specifically to Genai. Then later we saw that the shift in the development paradigm. Genai starts with foundational models adapting them with prompt engineering. where prompts are basically kind of data and code artifacts because it their prompts are developed for a certain model or a model version and chaining these various components together with techniques like RA or agible augmented generation to improve the output quality of your models. This was then followed by model adaptation and data practices. For example, we looked at fine-tuning techniques like supervised fine-tuning and RLHF which can be used as part of your MLOps pipeline to enhance performance for specific tasks and needs. Genai uses diverse data which includes prompts, grounding data, task specific data sets, human feedback and even synthetic data at times to kind of achieve the these are the tools which you can kind of turn to when you need to in your MLOX pipeline. After that the paper covered evaluation and monitoring which are also critical for genai workloads. We moved towards automated evaluation sometimes even using models or autoerators as judges as we saw in day one and we also looked at how some creating custom evaluation data sets for your business and the KPIs that you're tracking and continuously monitoring after the model is deployed continuously monitoring the model for skew drift and performance to ensure that it's operating as intended. deployment and governance handle the complexity of geni systems. But this also involves managing multiple artifacts for example prompts, chains, adapters and what have you via CI/CD, strict version control, optimizing foundational model deployments and extending governance across the entire lineage to make it easily trackable and traceable. a key section which we have added in this year's update would is basically adapting MLOps to agent or as we call it agent ops discovers the agent life cycle tool orchestration using a tool registry strategic tool selection also at scale agent specific evaluation observ observability for example memory and tracing as well as deployment pipelines for agent specific workloads and then all of that we concluded in the second half of the white paper we we discuss how all of this can be used and developed on by you on Vert. ex AI. It's basically a managed pipeline platform which provides tools across the full life cycle for example for discovery prototyping and services for tuning feature features for chaining and augmentation evaluation tools scalable endpoints with monitoring and robust governance features just to name a few. So that will that covers the summary and Paige would you like to take over the Q&A? Thank you. Yeah, thank you Anant. that was wonderful. so let's move quickly into the Q&A. I am overjoyed today to have a whole bunch of folks joining us from from Google Cloud to talk more about some of the projects that they've been building especially some of the Vert. Ex AI features that we just heard about. and we'll get started right away with our first question for Sora Tovari. It's great to have you here today. and I really really love all of the products that you have in your portfolio. you know, everything from kind of Kaggle and Collab, some of these beloved tools that that folks have been using throughout throughout the course as well as many many other production grade AI system tooling for Google cloud. So, excellent to have you. our first question is the pace of development in the field of agents has been tremendous especially productionizing agentic systems. What are some recent technologies that can help developers productionize agents? So, I mean yes there is a lot of excitement and thank you Paige and and it is great to join as well in this in this event. Hopefully many of you you who are watching and listening are finding it useful. the entire course if you look into the entire agent space there are many different like exciting things happening the most exciting I would say is LLMs are moving away from prompts getting wrapped around LLMs to actually these what we are calling as agents doing actual things and so within that there are a few things that you can think of which will be very important important one is frameworks. How do you manage and orchestrate u agents? this is where there are many open-source tools which are there like lang chain lang graph which is therei many many other things. U by the way this course happens just a week before Google cloud next. so we would also have a lot of announcements coming next week. So just hold on to that. we'll be talking about a lot of things which should be useful to this particular audience. then if you look into apart from the orchestration framework there is the model which is very important and one particular aspect of the model which is specifically important which is function calling because at the end of the day for the agents to be meaningful beyond just giving long responses or descriptive things etc. These agents would have to do actual things which means they need to interact with real world system and that means interacting with APIs or data sources etc. And so within that the quality of LLMs which are good in function calling will have a huge impact. Now obviously there are other aspects like for example the reasoning portion because when you are expecting the agents to do things they need to iterate on certain options meaning oh they did something just like us as human beings like we try to do something it doesn't work out then you iterate and you're like okay let me try this other thing etc and so on or you take a complex task and you unbundle it into smaller pieces and you co-executing all of them as well. those capabilities as well the thinking aspect of the models those will become important then access to APIs and data sources that is another critical piece which which is there and then the last one is the eval framework there is a lot of excitement I would say in the last couple of years relating to agents and many many demos have been done one reason why it hasn't picked up even though there is a lot of excitement about it is Because the demos work well but when you try to do things in real world or at high level of predictability that is what you want in like real grade production systems etc the quality does not hold up and so having automated eval systems will also be important and this includes things like autoator which needs to be there where you can have LLMs in a in a loop. actually if you think about because these agents can and because of the word agent they have agency to to do do things. you will have them operate in many different ways and so having kind of unit test or predictable frameworks to test these agents is not an easy way or it's not a scalable way so to speak. You cannot have human judges kind of labeling whether a particular agent worked this particular in this particular scenario or not etc in in hundreds of because it will become very expensive and un impractical very very quickly and so having potentially an LLM based eval framework for these agents is the other particular direction where I think there would be a lot of activity and so you would see across both Google products as well as other frameworks you'll see a lot of capabilities, tools, etc. across all of these different areas that are there and I hope to see much more exciting things happening with agents very soon. Awesome. Excellent. Thank you for the for the great answer and folks should remember that earlier in this week we learned all about agents and function calling and how you can use Gemini with those component systems to build your own agents. So really excited to see what folks create. next question for for Gabriella. traditional ML ops focuses heavily on managing training data and model versioning. How does this shift with generative AI where prompt engineering, fine-tuning data sets, rag, managing agent co tool chains become central? So, what new components or workflows are needed to help folks manage their MLOps pipelines for generative AI? Yeah, I think this question really hits at the heart of of the heart of how the MLOps landscape is evolving with generative AI. If we think about the core principles of MLPS, MLOps like automation, versioning, everything, code, data models, continuous integration and delivery, robust monitoring and fostering collaboration. These remain absolutely viral on generative AI as well. where things really change is what how is mentioned in this question is when because we are not often training models from the ground up anymore. So we usually start with these powerful pre-trained foundation models and and the game chiefs we usually we it's like less about initial training and more about how we adapt these big models and that means things like is mentioned in this question like prop engineering rack building agents etc. So basically this chip brings new first class citizens into our MLOps pipelines and this needs exactly the same level of rigorous management. So if we break it down for prompt we need to treat parts of the prompt like prompt templates as code versioning them test them deploy them reliably on verxi we are building tools into verxi used for this like prom management to store and iterate over prompts on the UI and also on the SDK. you can also lean on vari experiments and the evaluation services. so you can track different pro versions, log how well they actually work and even wire them into your CI/CD so updates get validated automatically. for building robots rack system you require to manage the data to embedded pipeline and the vector store itself. For instance, you can use for this Google cloud storage for source data for storing your source data. Vertex pipelines to automate the data ingestion and embedding generation and leverage vertx search as your as your manage vector database. So you don't have to run it yourself. and you can also use tools like BigQuery or cloud monitoring to to monitor your application. And now if we talk about agents well agents can get very complex too. So you have multiple steps and interactions. So you need ways to define them using open source frameworks. and then you can have you need to have a solid runtime to actually execute them. On on Verex AI we have agent engine which gives you this manage runtime. the nice thing is like is framework agnostic. So it helps you with deploying scaling and gives you observability with cloud trace logging and monitoring right out of the box. and for managing the actual tools the agents might use, you can use tools like artifact registry and actually the the whole u verxia platform and Google cloud is very flexible. So you could integrate cloud trace with open telemetry yourself if you wanted to build it like DIY style and I think Ellia is going to show how that works later. then if we think about fine-tuning especially what we've seen folks do a lot are parameter efficient finetuning or laura tuning. So on verex ai we also have our verx i tuning service which works really closely with verex experiments and evolution services. So you can keep meticulous track of everything what data you use the hyperparameters the results and then the adapter that you create and the models you deploy can get version and manage right into vertx AI model registry. So I think I'm covering all the points. It's a lot to cover and I love the the kind of attention to detail and the the focus on managing and version control for each one of these component pieces of a pipeline. we talked a little bit earlier in the week about how you know deploying agents feels a lot like distributed systems tasks. so so you know being being mindful and having a good kind of catalog of all of the all of the component pieces is really going to become mission critical for organizations. and I also really love just as a kind of quick shout out I love the open source evals framework called prompt fu which also works really really well with Google cloud and with the gemini APIs. cool. Next question. for Anon to Navan evaluating evaluating generative models goes beyond standard accuracy metrics. What practical techniques and tools are you using or developing within GCP for evaluating large language model outputs for quality? So things like fluency or relevance or lack of hallucination. also safety, cost effectiveness during development and continuous monitoring and production. And wow, that was a mouthful. so so how are you all thinking about evaluating generative models? So maybe I can start off and talk about evaluation in general and Ian we can I'll leave it to you for talking about agent eval. Okay perfect. So as learned as you guys saw in day one we talked a lot about evaluation I think using autoerators as judges has really taken off especially when it comes to evaluating these various dimensions as you mentioned and there's two aspects to that one is pointwise where you kind of look at the response and a prompt and say okay is this safe to go ahead with it is it fluent etc which does not so much depend on the prompt itself but often on the response and you can use pointwise metrics for this. But often another approach which works really well is pair-wise side by side as as we say it. because sometimes point-wise evaluations g give a very give a score which is too broad and you can have a lot of ties and as humans we often need to when we look at two different documents or even three different documents we often compare side by side to say okay hm I like this one more than the other one and that's where sideby-side comparison using auditor judges really helps a lot now these basic concepts can also be extended to evaluating agent trajectory ries and the final response as some of it which we covered in the day three agents. Ivan would you like to shed more color on this? Yeah. Yeah. So essentially as you said Anand I mean I'm as a judge for sure they are one of the important components in your evaluation toolbox that you want to use with this new generative application. But when we talk about agents agents they don't only you know generate the response they also use tool in order to generate that response. So that's why in vertex AI you know we integrated our toolkit for evaluating generative application with a set of matric matric metrics that allows you to understand if the agent is capable of you know taking the right path in when it decide to you know utilize some of the tools that you make available to the agent itself. So to the model itself and so with this with these metrics you can you can try to understand if the agents you know pick the right tool with respect to the user input or if it picks a set of tools in the right order or in any order. So you can also you know experiment with the variability of the the way the agents decide to use this tool. And the one important thing is that with the agent I mean this the tool selection of the agent or the way the agent uses this tool is one aspect but then you want you also want to combine like the response that the agent generate when you use a subset of tools right and in with in in this sense like the metrics that we provide today they are still like you know in a research space so that's why in with the vertexi geni experiment services, we give you the possibility to define your own custom metrics. So you you can define metrics that that are related to how the response you know kind of correlate with the tools that the agent decide to use. So in such a way that you know you you can you can evaluate in this sense if the agent not only picks the right tool with respect to the user input that he receive but also you know if use the tool in the correct way if it is if it is good in the reasoning part in picking these tools and generate the correct response. So this is very important just because again using a auto raider is just one way you can you can use to evaluate generative AI application but it has a it has its own pros and cons. So at the end of the day you really want to have a toolkit that allows you to implement your evaluation using autoated methods but also you know custom metrics. So it gives you this flexibility to adapt your evaluation with respect the application that you're trying to build. Yeah. And one small thing to add on to that talking about the toolkit argument. Another trend which has taken off is multimodal evaluation. So for those of you who are in Iclear, please drop by our booth. We are discussing text to image and text to video evaluation using using rubrics driven kind of where you kind of take take the original instruction make rubrics and use that to evaluate. This helps specifying it more to a certain task that also applies to text and video generation. So that'll be all. Thank you. Awesome. Excellent. And I I love the trend towards multimodal evaluation especially as people are getting more and more into challenges like video understanding audio understanding and even generating multimodal outputs from the models. next question for Socrates and IA. Looking at the full life cycle from data prep for fine-tuning and u monitoring deployed agents and models, what is currently the most significant bottleneck or challenge in applying MLOps principles effectively to generative AI projects and what advancements, platform features, best practices, new tools are most needed to overcome it? Thank you for this question. Up to this point, we heard everyone talking about evaluation, right? And this is one of the most important aspects if you want to bring something robust into production. So let's start with the technology aspect of things to do evaluation. First you need to prepare data. So one of the most critical bottlenecks that we have is how we prepare data for evaluation. Most of the customers in the market what they used to do is they used to incorporate the humans in the loop and what I mean is labelers that they go manually they check what is the the input that we need to to give to the LLM they generate an output and then they use to assess the models manually then we move also to LLM and synthetic data generation that it is the cost is lower than having humans but might not be that accurate or we might need to combine both. This problem becomes even worse whenever we have multimodality as you said right when where we have images, audio, video, how do you prepare this data for them? And if we think about more recent aspects like live streaming API, how can I evaluate live streaming? I interrupt a model while it talks and then how can I test this as well for agents? as we already mentioned one of the key aspects is how we call tools right so to create an evaluation data set it is not only about input query and expected output we need to track the whole tool chain what the function I call when I call and what was the sequence did I stay in the same topic so again the data preparation is difficult and nowadays what we have seen is we are trying to create systems that we record the interaction of a human with an agent with that way we gener generate the data and later on we can use them for evaluation. A very important aspect on this is memory management and memory management especially when we move in a multi- aent setup we start talking about graph databases and the valuation becomes even more difficult. Now similar topics that we discussed already is about tool management as well. Imagine in production systems we need to have authentication authorization of all these tools. It is not like in the classic machine learning that we had a model that we used to to to trigger and get a result back. Now we have tools that we trigger and we get the result back. So we need to orchestrate them with a very nice way. Next and I used to say that is MLOps or geni ops is not only about technology is people and processes as well. What I mean is we have new personas into the whole landscape. We had data scientists and machine learning engineers. Now we have prompt engineers that they need to be specialists on the topic of the application that they built. We have AI engineers who are responsible to know how to call a tool and how to integrate everything in a real application. We have devops and app devs that they need to integrate the whole back end that the front engineers and AI engineers have created into into an application. So as you can imagine this again can be a bottleneck. What I mean is the communication among all these different personas and to make them work like a very welloiled machine. And the last point that I want to raise is about knowledge. Knowledge of genai. This is a big bottleneck in the market. We have seen that most of the software engineers or business personas they want to use genai for everything. But actually they need to have the fundamental knowledge to somehow filter which applications can be solved with genai which not. So knowledge is very very very important tool. Awesome. And Ellia, is there anything you would like to add? Cool. Sounds sounds good. Like I I think that was very very comprehensive as an answer. Next question. How is GCP cloud AI simplifying the MLOps life cycle specifically for generative AI developers? and what are the the kind of key challenges that they face when operationalizing generative AI? so things like managing cost and latency. Mike and Avon, do you want to talk a little bit about this one? Yeah, sure. Thanks, Paige. I have a couple of thoughts about this. and then Ivonne, I can hand over to you and get your thoughts on this, too. yeah, I mean, as if you know, MLOps wasn't complicated enough, Genai comes along and makes it exponentially more complicated. So we're now trying to figure out how to manage this life cycle. I think one of the good things here is that is that Google has a tradition of being a developer focused company which we're a company of engineers. So we are really thinking about how to make a really developer friendly platform for Genai development. one important part of that obviously for Genai is prompt management. prompts are so important in Genai. I often think about a prompt and a model together as a as a unit. We talked about this in the white paper earlier this year. but you really have to manage the prompt itself as an important as a critical component to the application. So, we've introduced prompt versioning both in the Vert. ex AI SDK and in Vert. ex AI Studio. So, you can systematically manage your prompts as part of the application, programmatically manage the the prompts that you're deploying to your application in the same way that you manage the rest of your software supply chain and using the same software supply chain controls. So you can version all of the prompt parts of the application as much as the rest of the software. another part of it is that we're working on is infrastructure. obviously fine-tuning, Gabriela talked about fine-tuning and as part of the the the way of specializing and optimizing your JI application. we're trying to make that as streamlined as possible through things like Vert. ex AI custom jobs. So in a in a couple of lines of code, you can spin up your own fine-tuning pre-training jobs on Vert. Ex infrastructure using a managed cluster of of of nodes with multiple GPUs, multiple regions in the world. And you can do this all programmatically. Again, so we're abstracting away a lot of the the complication of deploying, you know, a cluster of training nodes and and acquiring the the GPU compute resources to do that kind of work. So again, that really takes away some of the the complication in the the productionization of a genai pipeline. and then when it comes to deployment, another part of that is, you know, obviously taking the model to production and making it available to your application. So you can deploy a pre-trained model, your fine-tuned model into the Vert. exi, Vert. Ex Ex AI prediction service that gives you an endpoint that your application can hit that has all of the infrastructure controls, security infrastructure support that you have that you need for a production application and you can do that just at the end of your Vert. exi custom job. Take the model, deploy it to Vert. ex AI prediction and it really again streamlines that whole process. And then as we're talking about agents, agents are obviously taking the whole the whole geni world by storm right now. You can deploy an agent, you know, a custom agent into agent engine using one of a number of frameworks that that that's available and you take advantage of the managed service underneath that to make your agent available in your Genai application. So we're really focusing a lot on these different parts of the process to streamline the whole Genai life cycle for MLOps application development. and Ivan are there anything you wanted to add to that as well? Yeah no I mean I just want to briefly touch two aspect that they ask in the question like what about the cost management latency optimization and the continuous evaluation. So I mean especially when you start you know building a generative AI application at scale using these models which is are which are powerful model they are they are cheap but they are not free right so I can understand why you are you want to monitor your cost and you want to also optimize the latency around them. So essentially the way you can do that is setting up some monitoring dashboards that allows you you know to count the input and the output token that the model generates and you know watch also the latency of a of a of your application and I mean when when you start building the application during the development for during the development phase for sure you can reuse some of the services that we mentioned so far. So for example with the vertex AI gener evaluation services you can compare different models and try to figure it out which is the the best one but also you know validate which is the cheapest one that you want to use compared to your your application. So if you have a simple task maybe you want to use a model that is cheaper rather than you know a big big reasoning models and so you can optimize this aspect during the development phase and then again when you go to production you can use some observability dashboard actually recently we release this integrated observability dashboard in in Vertex AI that you can leverage you know to track the you know the model the the number of token it consume and so like estimate your cost and the other aspect of continuous evaluation is that at the end of the day once you deploy an application this application will generate logs right in the past the log they were the prediction of your machine learning models today they are related to the tracing of the application that you generate especially if you have a chatbot or you have a or you have an agent. So what you can do is that you can collect this tracing and and then post like a post processing it in a way that you can structure your evaluation data set and run an evaluation job. And I think actually Aliyah in his last demo it will shows you it will show you how how you can do that and if you don't want to use the dashboard that we provide you can also use your own tools. So it will it will show you how to build a custom dashboard to monitor your application. So I think this br everything. Yeah, thank you both for the great answer. next question is for Sorab. looking ahead five years, how do you see MLOps for generative AI evolving to support enterprise applications particularly in areas like automated model retraining governance optimization? and how do you think Vertex AI will play into this transformation? So five years is a very long time period. if many of us think about what we were doing even 3 years back to now and the progression of deep learning or genai or AI in particular it has been quite a journey and I would say that the speed of change and acceleration will actually increase if you can imagine that. So predicting what will happen at 5 year time period is pretty risky and challenging. in terms of the the specifics of the of the question itself I would say we need to be and I think as many of the earlier speakers mentioned particularly I think Socrates was mentioning about having this knowledge or this thirst for knowledge of like what's happening what's changing etc. And that is where I would say tools like Collab, Kaggle, AI studio become very very useful and beneficial because you can get to if you and if you look at Kaggle right you have right now a K prize happening which is about software engineering automation aspects right and a very challenging goal which is which is there that can help give signals into I mean that's just one example there are many other competitions going on in Kaggle If you look into AI studio, I think earlier we were mentioning about some of the multimodel capabilities. There is a multimodel live API which is there which can be leveraged right now to kind of evaluate like when we are when I think Paige was mentioning about you can look at like machines can look at video they can look at text they can understand voice and compose all of these information to make the right judgment calls etc. whether it be in an auto evaluation kind of a framework or whether it be to actually do the task or to do that agentic work. So these things are all accessible right now. Again on collab side obviously there is a notebook functionality but as I think Paige was mentioning about the data science agent that's just one example where if you look back into if you had a large amount of data whether it be in your spreadsheet or whether it be in an alloy DB or more of a commercial database etc. And if you were to analyze information on top of that, you would be spending lots of time about data managing, cleaning, analysis, etc. And all of that could be automated away. So we already are seeing across even these three pieces, we already are seeing bits and pieces of how to connect the building blocks etc. And the speed of acceleration will just increase meaning if you have to project I mean I would say even 6 months a year down there will be multiple agents and there will be agent orchestrators will be running on top of some of these very close to semi-autonomous agents etc. The way to think about vertxi is whatever you see in the open source world or in the developer friendly world we provide the same thing on an enterprise or a managed setting etc. So lots of lots of acceleration. I kind of like even personally given the the part of my job the the type of things that I get to see is just fascinating. And the the other cool part which is happening is whatever is happening in the research side of like Google deep mind and many other places around the world. It is getting propagated at a unprecedented and truly I mean by that unprecedented word word that it is getting dispersed or distributed across the world in a very short period of time and so I would say just keep a close look at what is happening in in this particular space and be adaptable to the change which is there. Yep. I love that answer and I love the focus on kind of being flexible and adaptable and and kind of trying to to focus on outcomes and real problems as opposed to as opposed to you know trying to learn just one specific technology or one specific kind of model capability. because wherever we are with the models today in two weeks or even a month like we're all going to be blown away yet again. Yeah. And maybe let me just just add to that that typically in the past most of us have thought about oh here is an area here is a focus area where I will just continue iterating etc and so on this the shape or the landscape is just changing so quick quickly that we need to be really really adaptable to it right we can't be married to oh there is just one way in which we will go so for example oh I will just continue doing prompt rewrites to iterate and improve right there are now meta prompt writers where you describe what you want and then there are LMS which are optimized to write the prompts for the smaller models etc and so on right so there are things as well like those type of things that we we need to be open to yep it's awesome and I love like you mentioned I love getting to be embedded in kind of the the research and the the deployment at Google because it feels like we have a front row seat to to a lot of this progress excellent so next we are hiring as well yes we are hiring So if folks after you've learned and built with all of these generative AI tools Google is definitely hiring. next question from Kush what fundamental shift will happen in the software development life cycle through Agentic AI. So we already talked a little bit about one the data science agent and collab notebooks. What other things have been cooking up on the on the Google cloud and kind of broader developer community side? Socrates, do you want to go first? Yep. Sure, sure, sure. Well, software development, software development is nothing else than writing structured text, right? This is amazing because LLMs and agents can help us to do do to perform many tasks. First of all, we can start developing solutions extremely easy. I I I don't know if you have seen that currently we have canvases that we can write our prompts, see the code and see the preview of our code immediately. I used that last week to create Tetris in a few minutes right with Gemini. It was amazing. then after boosting the application development, the next and the most important thing is about the boosting the testing part. Testing is something that most of the people they are not really really good at, right? So this is a by leveraging agents it is a very easy way to to perform that. Now the next is about let's imagine that we have a scenario that we already have built a very complex repository with legacy code very very difficult so I have seen aic solutions that you can pass the whole code repository to them then they analyze your code they do the architecture design for your new query for example I want to add this functionality to my code and then they start generating the code they run the code they pass through the errors They resolve the errors and then they give you a complete solution. As you can imagine, this is amazing to boost the whole process and to come closer to the data science world, right? We have seen also solutions from agents that they prefer to generate the code for data scientists to create plots for example or to to create queries for data frames or we have seen even the natural language to SQL solutions that helps us a lot to create data queries for databases and this can make the life for data engineers extremely simple. So all these categories can help you to boost the whole development cycle. Ivan do you want to add something? Yeah. No, I mean what what I just would like to say is that especially now that we are seeing you know Gemini's models that we provide is getting better and better encoding. We are seeing that these kind of models they are already integrated in some of the most ID you know tools. So I think I think the the main the main thing is here here is the way like the interaction with this model for coding is changing the approach that as developers we have in in coding. So everyone right now is talking about this this idea of vibe coding. So which essentially is the concept where the user like use AI to you know write the code and then u based on simp simple the description that it provides to the tool and so and then he relies on the tool you know to to get the code that actually can integrate and use to build like the application that he has in mind. So to me it's just it's just the way you know we are approaching coding we are approaching software development together with these coding pairs that I mean is changing and I think in a positive way in some sense because as you said so is speed up a lot the the process of getting an application like a prototype up and running. Yeah, excellent answers. I've also been really impressed with some of the the developer tools externally. So things like cursor now supports Gemini continue. dev Dev, Rue, Klein, and the open- source space. Windinsurf from the Kodium team as well as Cody from the source graph folks. they're all adding support for Gemini, especially Gemini's, kind of longer context window, which can ingest full code bases. and it's been really magical to see how they can all kind of accelerate software development and even build in follow-up questions and more agent style approaches. I've really loved working with Rode for that. so hope excited to see more applications not just for the in IDE experience but also AI is applied to the full endto-end software development life cycle including things like code review and code performance and deployments. Awesome. so next question for Sorob, do you think there will be a market of agents created by software companies or digital creators? and you mentioned Google Cloud Next next week. So, so I'm wondering if there might be might be anything agent like announced there. I mean, we will be talking about agents quite a lot next week including in Thomas Kuran's keynote who is the Google cloud CEO. but going back to the question, I think the answer is a big yes. you will see multiple agents across multiple frameworks as we talked earlier like there are different frameworks which are there different data sources. There is different quality aspects etc. There will be a heterogeneity of agents which will be there. There will be value which will be attributable to agents depending upon how good they perform, how much value do they bring in and because of those reasons there will be a marketplace. one thing I would call out which I don't think is getting that level of attention and I feel will become very very important in the agent space is security. As agents start having more capabilities and ability to do things then the simpler example I would say is as there were apps in the app store on your phone whether it be Android or iPhone where you have to go through an approval process then there is like controls on cameras and battery and stuff like that etc. In a similar way, if you think about agents can do much much more and so whether there are inherent by design kind of like malicious activities or whether through other attributes some bad outcomes could be created right those things authentication is another thing access to content which is another thing right so that entire space I I just want to call out and put a plug for all our developers who are thinking who are participating in this in this course and are thinking about building agents to think along that particular axis because that will be an important play and yes there will be a marketplace with exchanges etc happening as well. Yeah, I've loved seeing the I've loved seeing the the investment that Google cloud is making in things like MCP servers and then also these kind of robust security approaches towards agent agent interactions. Yeah. the the Stripe team also had a really great example a couple of weeks ago where they were using agentto agent authenticated payments so agents could kind of authenticate and pay other agents as well. So so lots of really interesting work needed to happen in that space to make it enterprise ready. and hopefully some of the folks on the call will be inspired to build businesses around this or to to join existing businesses to make that happen. Yeah. Awesome. Thank you for the great question, Peter. And with that, we are going to head over to the code labs. Ellia, I I'm very excited to learn more. I hear that this is a live demo and so we will be crossing our fingers and hoping to the demo gods that everything will be going as attended. Is that correct? Yeah, that's correct. Fingers crossed. Yeah. so I'll share my screen. and we can start with a really really quick presentation on a resource we created to help you accelerate your agent development journey essentially which is called starter pack. So we are going to start with few numbers which is essentially the fact that it took us from around three months for us which are experienced GCP developers to deploy a dummy genai agent in production. So what we see with our cloud Google cloud customers is instead that it takes around 3 to nine months to do the same process and so this is typically a long process and so we thought why is that you know why what are the reason that are leading to this long time to develop an agent in production. so we found out that there are some common challenges and you know we discovered also by actually talking with our customers in the last two years that typically creating the first agent and you know creating the first prototype is typically the the easiest part. So downloading a sample starting you know a notebook and things like these are pretty easy. but then when it when it when it comes time to go to production, this is where customers and developers start encountering challenges and so there are some common challenges we we started to see. So the first challenge is around customization. So customizing the agent based on your business needs or business logic. So many agents will for example consume data. So how do we make sure this data is accessible, high quality and fresh is an open question. And so and then we discussed also previously about security and compliance. So things related to data privacy, access control, adversarial attack mitigation. We discuss around evaluation. So measuring the performance of the agent, building confidence on the agent is an art topic and then also on generating synthetic data which can then be used for evaluation purposes and then also for tooling purposes. Then we have the deployment part. so which which is touching areas like how do we integrate our agent into a into a UI to make a product essentially and then things like CI/CD pipelines testing rapid iteration and rollback mechanism are still challenges when it comes to building agents and finally infrastructure so creating scalable and robust infrastructure to deploy the agent is an open challenge and finally observability so the areas like how do we monitor the performance of the agent how do we build insights with our agents so that we can iterate back and start again the development process. And so hopefully these are challenges that are also resonating with you as well. And to support customers and developers in into solving a good part of these challenges we created this starter pack resource which is essentially a a collection of production ready agent templates designed to accelerate time to production from months to weeks essentially. So this cover all the areas we discussed earlier. so like from deployment and operations to customization observability you you need essentially all you need to get to production quickly. And so like we discussed like the deployment operation part where we have things like an API server UI playground and a set of CI/CD and Terraform samples ready for you to start building your agent. the customization part where we offer a series of readytouse AI patterns and the observability part and evaluation part as well and so I think like now we can directly jump into a demo where we are going to showcase this resource here so like I can share a different screen now so I can share with you my terminal command okay so fingers crossed everything will fine with the demo. So hopefully you can see my terminal command and typically the journey starts into GitHub. So we have an open open-source repository called Google Cloud Platform/ Aentarterbug. You can go there and start using it as a quick start as well. It's essentially a Python package you can install and start using it like this. So I already done it in my terminal. So we can start using it directly. For example, let's say I want to create a new agent and I want to call my agent guide to MLOps. So I will hit enter. This will guide me into our inter interactive UI where I will be able to select one of the agents I want to start using to build agents and of course like keep keep this space watch this space really closely because at cloud next we are going to introduce more agents here. so let's say I want to use a dummy agent and I will be able to choose like how I want to deploy the agent as well. In this case, I'm going to use Vert. Ex AI agent engine, which is a manage runtime to deploy the agent. So, I'll click next. I'll it with all the defaults. I will verify that I have enough permissions to call Vert. exai. And in a couple of seconds, you will see we created a new agent. So like we can actually see for example the readme of the agent you know and this is actually creating the full project structure including the code notebooks test may file and so on and so forth. So I already have a UI open where you will be able to actually see the agent and you know we can actually start asking question directly to it and you know you can actually start developing changing things on the code iterating until you're ready to actually deploy this agent to production. I started already the process just for the sake of time and I created a repository where I push the agent we created earlier. And so like the really interesting part of this repository is that you can see that it contains some CI/CD pipelines that are actually deploying the agent to production. So you can see now we have moving to cloud build and in cloud build you will see that there is a pipeline that is deploying the agent into a staging environment running things like load test against agent engine and then you know we can actually add all the report of all the load test how the agent is performing if it's have enough latency and so on and so forth and then when we are ready we can actually deploy to production with one button and and start having our users testing it and using it in production. The journey doesn't finish here because when the agent is in production, we want to understand how the users are using it. And so what this is where the starter pack is offering automated observability. so like you'll be able to go into cloud trace and into cloud trace for each call the user is making to the agent you'll be able to select one of the calls and understand how the user is actually using the agent. So like what what are the steps that the agent perform and for each step what what were the specific calls the user and the agent actually have done you know this is really important to build insights all the data you have here will then land in into bequery so that you will be be able to create dashboards you will be able to create evaluation data set and you know you we even offer a lookers to your dashboard so that you can actually understand like how the agent is performing over time and of course this is just a template So everything we are providing here you can take it edit it according to your needs and then start building agents as well. So hopefully it is concludes like the demo. I loved seeing that. It looked very very simple to go from GitHub to having an agent that you could test and deploy and also loved being able to see the logs and telemetry for the agent to understand how it's performing over time what kinds of question user users are asking it and things like costs and latency. So, thank you so much for for the demo. It was great. and also wonderful to to see how quickly all of those things could be done with the agent starter pack. Awesome. Excellent. So, thank you so much to all of our wonderful Q&A presenters. I learned a lot this segment and I think we're all very inspired to understand how do you incorporate Emilops into all of our apps and all of our practices. I'm going to go ahead and go to the next section, which is all of our pop quiz questions. everybody get a pen and paper. and we'll be going through a few questions before getting into into the wrap for today and for our generative AI course. So, first question, which of these is not a core practice of MLOps for generative AI applications as discussed in the white papers? Is it a prompt engineering and evaluation as an iterative cycle? b data validation, model evaluation and model monitoring. C training a foundation model from scratch or D managing and versioning prompt templates, chain definitions and external data sets. Which of these is not a core practice of MLOps for generative AI? I'm going to count down five 4 3 2 1. and C is not a core practice of MLOps for generative AI applications. We're we're focused on all of the infrastructure around the model, not training a foundation model from scratch. Question number two, what is a prompt template in the context of generative AI? Is it A, a simple text input from the user, B, a set of instructions and examples with placeholders for user input, C the foundation model itself, or D the final output generated by the model. I'm going to count down five 4 3 2 1 and the correct answer is B. A set of instructions and examples with placeholders for user input. Question number three, what is the purpose of chaining in generative AI applications? Is it A to maintain recency in the model's outputs? B to avoid hallucination and maintain recency in the model's outputs, C to increase the complexity of the model, or D to reduce the efficiency of the model. gonna count down five, four, three, two, one. And the correct answer is B. To avoid hallucination and maintain recency in the model's outputs. Question number four. Why is evaluation a crucial step in the development of generative AI systems? And we've been talking about this all week. So hopefully folks get this question right. is it to ensure the model is deployed to the correct infrastructure? Is it B to optimize resource utilization and reduce latency? Is it C to track the lineage of data and model versions or D to measure the quality and effectiveness of the model's outputs? Going to count down 5 4 3 2 1. And evaluation is a crucial step in measuring the quality and effectiveness of the model's outputs. Hopefully everybody got this right and are already thinking about how you can create your own evals. Question number five, which Vertex AI product allows for recurrent execution of evaluation product evaluation jobs in production skew and drift detection processes? Is it A, Vertex AI model monitoring, B, Vertex AI pipelines, the C Vertex AI feature store, or D, Vertex AI model registry? I'm going to count down five 4 3 2 1 and the correct answer is B. Vertex AI pipelines. And with that, it is hopefully everybody got 100% correct. we are going to head to the final piece of our Kaggle Generative AI intensive course. We have wrapped up all of our kind of in-person YouTube streams and now hopefully you have all of the tools in your toolkit to do our final segment which is the exciting capstone project. so for this capstone project, you can level up your skills and build your your engineering portfolio with a real world AI engineering project. this competition starts today and ends on April 20th. And your goal is to create a notebook demonstrating a use case using some of the generative AI capabilities that you've learned from this week's course, whether it's agents, vector databases, embeddings, or all of the above. and for bonus points, you can create a blog post or a YouTube video to share everything about what you've learned, and about your project. evaluation and submission details will be shared today. so make sure to pay attention on Discord and via email. all of our participants will receive a Kaggle badge and a certificate on their Kaggle profiles by the end of the month, by the end of April. and you can either work individually or you can form teams. So there's no need to go alone. You can you can do this with a group of your friends and and we are so looking forward to seeing what you build. This is very very exciting and the best way to learn is by applying. So hopefully everybody has been thinking of something that they'd like to build as they've been kind of tackling all of the course curriculum throughout this week. and we're looking forward to seeing what you create. So, thank you so much for tuning in this week. I I I think that it's been magical to see everything. you know, all over the world, over a quarter of a million of y'all kind of sharing sharing all of your excitement. and this is the part of the job that that makes everything worth it. Anant, do you want to add anything? Yes. And the winners of this of the capstone project will also be amplifying your capstone projects on Kaggle and Google. coms. So yeah, be looking forward to see what you discuss. Yeah. So excellent. I can't wait to to take a look and also just want to say personally thank you so much again to all of our wonderful course moderators, all of the people behind the scenes who were building things to make sure that that you know all of y'all would have the the tutorials and the educational materials. It really does take a village to make it to make it all happen. So thank you so much. have a beautiful weekend ahead. and we appreciate you and and are very excited to to have you as part of this course and this capstone project.